<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[最短路径算法]]></title>
    <url>%2F2018%2F08%2F19%2Fshortest-path%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍最短路径的相关概念、单源最短路径和所有结点对的最短路径。本文的完整代码可以在我的github找到。 概念对于一个带有权重的有向图$G=(V, E)$和权重函数$w: E \rightarrow R$，该权重函数将每条边映射到实数值的权重上。图中一条路径$p=$的权重$w(p)$是构成该路径的所有边的权重之和： $$w(p)=\sum{i=1}^{k}w(v{i-1}, v_i)$$定义从结点u到结点v的最端路径权重$\delta(u, v)$如下： $$\delta(u, v) = \begin{cases}\min{w(p): u \leadsto v} &amp; 如果存在一条从u到v的路径 \\infty &amp; 其他\end{cases}$$ 从结点u到结点v的最短路径定义为任何一条权重为$w(p)=\delta(u, v)$的从u到v的路径p。 最短路径问题由几种 变体，接下来我们主要讨论单源最短路径问题和所有结点对的最短路径问题。 单源最短路径单源最短路径问题：给定$G=(V, E)$，找到从源结点$s \in V$到每个结点$v \in V$的最短路径。有时单源最短路径问题可能会包含具有负权重的边。但如果图$G=(V, E)$不包含从源结点s可以到达的权重为负值的环路，则对于所有的结点$v \in V$都有最短路径（可能为无穷）。如果环路的权重为正值，则此环路必定不存在于最短路径上；如果环路的权重为负值，则最短路径没有定义，因为我们总是可以尽可能多的包含此环路，使其最短路径为$-\infty$；如果环路的权重为0， 则必然有不包含环路的最短路径。 最短路径通常同前驱子图来表示，我们记录每个结点v的前驱结点$v.\pi$。我们定义前驱子图$G\pi=(V\pi, E\pi)$，其中$V\pi={v \in V: v.\pi \neq NIL} \bigcup {s}$，有向边集合$E_\pi={(v.\pi, v) \in E: v \in V.\pi -{s}}$。 我们定义图如下：123456INF = float('inf')class Graph: def __init__(self, n): self.n = n self.e = [] self.w = [[0 if i == j else INF for j in range(n)] for i in range(n)] Bellman-Ford算法Bellman-Ford算法逐渐通过松弛操作降低从源结点s到结点v的最短路径的估计值$d[v]$，直到达到最短路径$\delta(s, v)$为止。松弛操作的python的代码如下：1234def relax(g, p, d, u, v): if d[v] &gt; d[u] + w[u][v]: d[v] = d[u] + w[u][v] p[v] = u Bellman-Ford算法的python代码示例如下：12345678910111213141516def bellman_ford(g, s): d, p = [], [] for i in range(g.n): d.append(INF) p.append(None) d[s] = 0 for i in range(g.n - 1): for u, v in g.e: # 松弛操作 if d[v] &gt; d[u] + g.w[u][v]: d[v] = d[u] + g.w[u][v] p[v] = u for u, v in g.e: if d[v] &gt; d[u] + g.w[u][v]: return False, p, d return True, p, d 松弛操作的原理也很简单，因为边(u, v)存在，所以最短路径的估计值d[v]必定小于或者等于d[u] + w[u][v]。d[v]相当于是最短路径$\delta(s, v)$的一个上界。Bellman-Ford算法每次对所有的边进行松弛操作， 共循环|V|-1次，使得d[v]最终成为最短路径。至于其准确性简单说明如下。首先我们引入定理： 最短路径的子路径也为最短路径。 即如果从$v_0$到$v_k$有一条最短路径$p=( v_0, v1, …, v{k-1}, v{k})$，则$p{ij}=(vi, v{i+1}, …, v_{j-1}, v_j)$也是从$v_i$到$vj$的最短路径。那么在第一次循环时，对于$p{sv}=(s, v)$的结点v，即最短路径只有一条边的结点，d[v]即为其最小距离。对于最短路径只有两条边的结点在第二次循环时即可求出，依次类推，因为s到v的简单路径上最多包含|V|-1条边，则循环|V|-1次即可求解。若最短路径不包含权值为负的环路，则最短路径算法返回True。容易知道，此算法的时间复杂度为O(VE) Dijkstra算法Dijkstra算法同样可以解决带权重的有向图的单源最短路径问题，不过它要求所有边的权重都为非负值。Dijkstra算法维护一个结点集S，从源结点s到达该集合的每一个结点的最短路径均已找到，每次从V - S中取出最短路径估计最小的结点u将其加入S。其python代码示例如下：123456789101112131415161718192021222324252627282930def find(d, flag): index, min_dist = -1, INF for i in range(len(d)): if flag[i] is False and d[i] &lt; min_dist: index = i min_dist = d[i] if index == -1: return None return indexdef dijkstra(g, s): d, p, flag = [], [], [] for _ in range(g.n): d.append(INF) p.append(None) flag.append(False) d[s] = 0 while True: u = find(d, flag) if u is None: break flag[u] = True for v, weight in enumerate(g.w[u]): if weight &lt; INF: if d[v] &gt; d[u] + weight: d[v] = d[u] + weight p[v] = u return p, d 从代码中，易知其时间复杂度为$O(V^2)$。但如果像Prim算法那样使用设计良好的最小优先队列，并使用邻接链表法，则可以使其使见复杂度降低至O((V+E)logV)，即O(ElogV)。 有向无环图的最短路径算法对于有向无环图我们可以利用拓扑排序来优化最短路径的时间，其伪代码示例如下：12345678DAG-SHORTEST_PATH(G, w, s) 利用深度优先搜索对G进行拓扑排序 初始化父亲数组p，距离数组d for u in 拓扑排序后的结点 for v in G.adj[u]: if d[v] &gt; d[u] + w[u][v] d[v] = d[u] + w[u][v] p[v] = u 按照拓扑排序进行遍历的原因如下：如果存在一条边(u, v)，则结点u的拓扑排序在结点v的前面。当前循环的结点到达v时，d[v]已经是最短路径了，因为在此之前所有能到达v的结点u已经被遍历了。依此类推，当遍历结束，最短路径就都求出来了。当使用邻接链表的表示方法时，其时间复杂度度为O(V+E)。但当使用邻接矩阵的表示方法时，其时间复杂度为$O(V^2)$。使用邻接矩阵法的有向无环图的最短路径算法的python代码如下：1234567891011121314151617181920212223def dfs(g, u, f, flag=None): flag = flag if flag else g.n * [False] flag[u] = True for v, weight in enumerate(g.w[u]): if flag[v] is False and weight &lt; INF: dfs(g, v, f, flag) f.insert(0, u)def dag_short_path(g, s): d, p = [], [] for i in range(g.n): d.append(INF) p.append(None) d[s] = 0 f = [] dfs(g, s, f) for u in f: for v, weight in enumerate(g.w[u]): if weight &lt; INF: if d[v] &gt; d[u] + weight: d[v] = d[u] + weight p[v] = u return p, d 从代码中可以看到我们并没有对整个图进行深度优先遍历，我们只对从源结点s出发的结点进行了深度优先遍历。因此我们所得到的拓扑排序f为以s为根的深度优先树的拓扑排序。因为如果源结点s可以到达结点v，则v必定在拓扑排序中。上述实现因为使用邻接矩阵，因此时间复杂度为$O(V^2)$。 所有结点对的最短路径平方重复法所有结点对的最短路径算法可以使用动态规划的方法，其基本思路如下：假设从结点i到j存在一条最短路径p，且p最多包含m条边，假定没有权重为负值的环路。设路径p上结点j的父结点为k，即$p=(i, …, k, j)$，则$p^{‘}=(i, …, k)$为从结点i到k的最短路径。则递推式为 $$\delta(i, j)=min(\delta(i, k) + w[k][j]) \forall(k, j) \in E$$则按照这个思路实现的伪代码如下：12345678910EXTEND-SHORTEST-PATHS(L, W) n = L.rows let L&apos; be a new n*n matrix for i =1 to n for j = 1 to n L&apos;[i][j]=INF for k = 1 to n L&apos;[i][j] = min(L&apos;[i][j], L[i][k]+W[k][j]) return L&apos; L初始化为权重矩阵W。因此需要进入这样的函数|V| - 1次，因此其时间复杂度为$O(V^4)$。这样的过程类似于矩阵的乘法，对于矩阵的乘法我们有以下的伪代码：12345678910SQURE-MATRIX-MULTIPLY(A, B) n = A.rows let C be a new n*n matrix for i = 1 to n for j = 1 to n C[i][j] = 0 for k = 1 to n C[i][j] = C[i][j] + A[i][k] * A[k][j] return C 比较之下，发现两者非常相似。将min替换成+，将+替换成*，即完全一致。对于矩阵乘法我们有: $$L^{(1)} = W \L^{(2)} = W W \L^{(4)} = W^2 W^2 \L^{(8)} = W^4 W^4 \…\L^{(2m)} = W^m W^m \$$因此我们在求解最短路径时，可以利用上述性质，因为只要运行EXTEND-SHORTEST-PATHS函数|V|-1次必然得到最短路径，即$W^{n-1}, n=|V|$必然为最短路径，且之后再次运行此函数，最短路径矩阵将不会改变。因此我们只需要利用上述的原理即可得到如下的伪代码：12345678910FASTER-ALL-PAIRS-SHORTEST_PATHS(W) n = W.rows L[1] = W m = 1 while m &lt; n - 1 let L[2m] be a new n * n matrix L[2m] = EXTEND-SHORTEST-PATHS(L[m], L[m]) m = 2m return L[m] 因为我们只需要得到$W^{n-1}$，而$W^{m} = W^{n-1} \forall m&gt;n-1$。此时时间复杂度降低至$O(V^3logV)$。其python代码示例如下：12345678910111213141516171819202122232425262728def extend_paths(l, w, p): n = len(l) y = [[INF for _ in range(n)] for _ in range(n)] pp = [[None for _ in range(n)] for _ in range(n)] for i in range(n): for j in range(n): pp[i][j] = p[i][j] for k in range(n): if i == j: y[i][j] = 0 pp[i][j] = i else: if y[i][j] &gt; l[i][k] + w[k][j] and k != j: y[i][j] = l[i][k] + w[k][j] pp[i][j] = p[k][j] return y, pp def all_short_paths(g): n = g.n l = g.w p = [[i if g.w[i][j] &lt; INF else None for j in range(n)] for i in range(n)] m = 1 while m &lt; n - 1: l, p = extend_paths(l, l, p) m = 2 * m return l, p 需要指出的是以上代码虽然能正确的得到最短路径d，但是记录前驱结点的数组p不一定准确。$p[i][j]=k$表示从源结点i到j的路径中j的前驱结点为k。数组p只有在图中存在权重为0的环时才可能不准确，大部分情况下都是准确的。 Floyd-Warshall算法Floyd-Warshall算法使用了一种不同的动态规划方法，这种动态规划方法更加抽象，如果让我子集设计一个所有结点对的最短路径算法，我可能倾向于会想到上面的重复平方法，而非这种方法。但事实证明，设计更有抽象的最优子问题，往往会得到更高效的动态规划算法。 Floyd-Warshall算法考虑的是最短路径上的中间结点。一条简单路径$p={v_1, v_2, …, v_l}$上的中间结点即指p上除了$v_1$和$v_l$的任意结点。Floyd-Warshall算法定义最有子问题如下： 对于任意结点i和j，从结点i到结点j的最短路径p的中间结点全部取自${1, 2, …, k}$, 其中结点的全集为$V={1, 2, 3, …, n}$ 如果从i到j的最短路径的中间结点均取自${1, 2, …, k}$，则有两种情况： 如果k不在最短路径上，则从i到j的中间结点取自结点集${1, 2, …, k-1}$的最短路径也为从i到j的中间结点取自结点集${1, 2, …, k}$的最短路径。 如果k在最短路径上，则将路径分解为从i到k再到j，即$i \rightsquigarrow k \rightsquigarrow j$，则从i到k的一条最短路径取自${1, 2, …, k-1}$，同样从k到j的一条最短路径也取自${1, 2, …, k-1}$。 子问题分解完成。当k=0时，表示从结点i到j一条最短路径不包含任何的中间结点。记$d{ij}^{(k)}$表示从结点i到j的所有中间结点取自${1, 2, …, k}$的最短路径的权重。则$d{ij}^{(0)}=w{ij}$。其$d{ij}^{(k)}$的递归式如下： $$d{ij}^{(k)} = \begin{cases}w{i, j} &amp; k = 0 \min(d{ij}^{(k-1)}, d{ik}^{(k-1)}+d{kj}^{(k-1)}) &amp; k\geq 1\end{cases}$$当k=n时，矩阵$D^{(n)}=(d{ij}^{(n)})$接触所有结点对的最短路径。我们可以再计算矩阵$D^{(k)}=(d{ij}^{(k)})$的同时计算前驱矩阵$P^{(k)}$。与前面类似，$p{ij}^{(k)}$表示从结点i到j的一条所有中间结点都取自集合${1, 2, …, k}$的最短路径上j的前驱结点。则当k=0时，从i到j的最短路径没有中间结点，则有 $$p{ij}^{(0)} = \begin{cases}NIL &amp; i=j或w{ij}=\infty\i &amp; i\neq j或w_{ij}\lt\infty \\end{cases}$$其递归式如下： $$p{ij}^{(k)} = \begin{cases}p{ij}^{(k-1)} &amp; d{ij}^{(k-1)} \leq d{ik}^{(k-1)} + d{kj}^{(k-1)}\p{kj}^{(k-1)} &amp;d{ij}^{(k-1)} \gt d{ik}^{(k-1)} + d_{kj}^{(k-1)} \\end{cases}$$根据以上的递归式，其python实现如下：123456789101112131415161718192021def floyd_warshall(g): n = g.n d, p = [], [] for i in range(n): d.append([]) p.append([]) for j in range(n): d[i].append(g.w[i][j]) if i != j and g.w[i][j] &lt; INF: p[i].append(i) else: p[i].append(None) for k in range(n): for i in range(n): for j in range(n): if d[i][j] &gt; d[i][k] + d[k][j]: d[i][j] = d[i][k] + d[k][j] p[i][j] = p[k][j] return d, p 相比于重复平方法，floyd_warshall算法更加简洁和高效。其时间复杂度为$O(V^3)$，空间复杂度为$O(V^2)$。而且其能够克服上述所说的权值为0的环路的缺点。 Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>最短路径算法</tag>
        <tag>Bellman-Ford算法</tag>
        <tag>Dijkstra算法</tag>
        <tag>平方重复法</tag>
        <tag>Floyd-Warshall算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小生成树]]></title>
    <url>%2F2018%2F08%2F19%2Fmin-spanning-tree%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍最小生成树的概念以及最小生成树常用的两种算法：Kruskal算法和Prim算法。本文的完整代码可以在我的github找到。 概念对于一个连通加权无向图G=(V, E)和权重函数$w: E \rightarrow R$，希望找到一个无环子集$T \subseteq E$，使得T中包含V中所有的结点，且$w(T)=\sum_{(u, v) \in T}w(u, v)$的值最小。则(V, T)为最小生成树（最小权重生成树）。 定义无向图$G=(V, E)$的一个切割(S, V-S)时集合A的一个划分。如果一条边$(u, v) \in E$的一个端点位于集合S，另一个端点位于集合V-S，则称该条边横跨切割(S, V-S)。如果集合A中不存在横跨该切割的边，则称该切割尊重集合A。在横跨一个切割的所有边中，权重最小的边称为轻量级边。 最小生成树的可以使用贪心策略来生成，这个贪心策略由下面的通用方法来表述。该方法在每个时刻生长最小生成树的一条边，并将其加入到边集合A中，A遵循以下循环不变式： 在每遍循环之前，A时某棵最小生成树的一个子集 在每一步，我们要训责一条边(u, v)，将其加入到集合A中，且使得A仍然遵守此循环不变式。由于这样的边没有破坏A的循环不变式，称这样的边对于A是安全的。尊重集合A的切割的轻量级边对于A是安全的。最小生成树的通用方法伪代码如下：12345GENERIC-MST(G, w) 初始化A为空集 while A没有形成一个生成树 找到A的一条安全边，加入到A中 return A Kruskal 算法Kruskal算法的基本思路如下： 初始化边集合A为空集 初始化每个结点为一棵单结点的树，利用不相交集合来实现 对所有的边按照升序排列 取出按照升序排列的一条边(u, v)，判断u和v是否属于同一棵树，如果不同，则将u所在的树和v所在的树并起来，将边加入到集合A中。 Kruskal算法始终维护一个森林，森林包含图G所有的结点，每次找到一条连接森林中的两棵树的权重最小的边，将其添加到边集合中。我们假设这样的边为$(u, v)$, $C_1, C2$为其连接的两棵树，则(u, v)必定为一条安全边。令$(V{C1}, V-V{C_1})$为一个划分，则此划分尊重集合A，则边(u, v)为一棵轻量级边，因此(u, v)为一条安全边。 在Kruskal算法中我们需要使用不相交集合，我们用p、rank数组表示其父亲和秩。其定义如下：123456789101112131415161718def union(x, y, p, r): return link(find_set(x, p), find_set(y, p), p, r)def link(x, y, p, r): xRoot = find_set(x, p) yRoot = find_set(y, p) if r[xRoot] &gt; r[yRoot]: p[yRoot] = xRoot else: p[xRoot] = yRoot if r[yRoot] == r[xRoot]: r[yRoot] += 1def find_set(x, p): if p[x] != x: p[x] = find_set(p[x], p) return p[x] 在本次示例中，我们使用邻接链表来表示无向图，其python代码示例如下：123456789101112class LinkGraph: def __init__(self, n): self.n = n self.adj = [[] for _ in range(n)] self.e = []# 相当于将LinkGraph当成数据结构使用。def insert(g, e): for u, v in e: g.adj[u].append(v) g.adj[v].append(u) g.e.append((u, v)) Kruskal算法的python代码如下：1234567891011121314151617def kruskal(g, w): A = [] # make set p = list(range(g.n)) r = g.n * [0] tmp = [] for i, (u, v) in enumerate(g.e): tmp.append((i, w[u][v])) tmp = sorted(tmp, key=lambda x: x[1]) for i, iw in tmp: u, v = g.e[i] if find_set(u, p) != find_set(v, p): union(u, v, p, r) A.append((u, v)) return A Kruskal算法的时间复杂度为$O(ElogV)$。 Prim算法Prim算法很朴素，它维护一个边集合A，A中结点集为$V_A$，它直接使用$(V_A, V - V_A)$作为一个划分，则此划分必定尊重集合A。我们只要找到横跨该切割的一条轻量级边(u, v)，则(u, v)对A来说是安全。值得说明的是A中的边始终构成一棵树。Prim算法的python代码示例如下：12345678910111213141516171819202122232425262728293031323334def find(d, flag): u, min_dist = -1, float('inf') for i in range(len(d)): if flag[i] is False and d[i] &lt; min_dist: u, min_dist = i, d[i] if u == -1: return None return udef prim(g, w, r): INF = float('inf') p = [] flag = [] d = [] for i in range(g.n): p.append(None) flag.append(False) if r == i: d.append(0) else: d.append(INF) while True: u = find(d, flag) if u is None: break flag[u] = True for v in g.adj[u]: if flag[v] is False and w[u][v] &lt; d[v]: p[v] = u d[v] = w[u][v] return p 以上代码的基本思路如下： 初始化数组flag、p和d。flag为一个状态数组，用以表示结点是否已经在$V_A$中，若$flag[u]=False$，则结点不在$V_A$中。p用来保存结点的父结点，$p[v]=u$则v的父结点为u。d用来保存结点到$V_A$中所有结点的最短距离。我们初始化源结点r的距离为0。 while循环的每一次循环，都先找一个结点u，则$(p[u], u)$为一条轻量级边，将结点加入$V_A$，即设置flag[u]=True。扫描u的所有邻接结点v，若结点v不在$V_A$中，则将其父亲设为u，即将边(u, v)加入到了A中。同时更新v到结点集$V_A$的最短距离。 分析下上面算法的时间复杂度，首先初始化时间为O(V)，while循环了V次，find操作的时间复杂度为O(V)。for循环了2E次，则总时间复杂度为$O(V+V^2+2E)$，即O(V^2)。在算法导论中引入小根堆，使得其时间复杂度为O(ElogV)。不过这依赖于小根堆的设计，下面的python代码是我用小根堆实现的Prim算法。其时间复杂度依赖于数组的index方法，如果index方法的时间复杂度为线性，即为O(V)，则下面的算法的时间复杂度反而达不到O(ElogV)，反而时间复杂度为O(EV)。如果良好的设计使得for循环中除了维护小根堆性质的rise操作外，时间复杂度均为O(1)，则时间复杂度能达到O(ElogV)。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485def prim(g, w, r): INF = float('inf') q = Heap() p = [] for i in range(g.n): p.append(None) if r == i: q.heappush(0, i) else: q.heappush(INF, i) while q.arr: _, u = q.heappop() for v in g.adj[u]: if v in q.indexs: index = q.indexs.index(v) if w[u][v] &lt; q.arr[index]: p[v] = u q.arr[index] = w[u][v] q.rise(index) return p# 小根堆的实现class Heap: def __init__(self, arr=None): self.arr = arr if arr else [] self.indexs = list(range(len(arr))) if arr else [] if self.arr: self.buildheap() def heappush(self, item, index): self.arr.append(item) self.indexs.append(index) i = len(self.arr) - 1 parentpos = (i - 1) // 2 while i &gt; 0 and self.arr[parentpos] &gt; self.arr[i]: self.arr[parentpos], self.arr[i] = self.arr[i], self.arr[parentpos] self.indexs[parentpos], self.indexs[i] = self.indexs[i], self.indexs[parentpos] i = parentpos parentpos = (i - 1) // 2 def buildheap(self): n = len(self.arr) for i in reversed(range(n // 2)): self.heapify(i) def heapify(self, i): n = len(self.arr) left = 2 * i + 1 right = left + 1 smallest = i if left &lt; n and self.arr[left] &lt; self.arr[smallest]: smallest = left if right &lt; n and self.arr[right] &lt; self.arr[smallest]: smallest = right if smallest != i: self.arr[smallest], self.arr[i] = self.arr[i], self.arr[smallest] self.indexs[smallest], self.indexs[i] = self.indexs[i], self.indexs[smallest] self.heapify(smallest) def rise(self, i): n = len(self.arr) parentpos = (i - 1) // 2 while i &gt; 0 and self.arr[parentpos] &gt; self.arr[i]: self.arr[parentpos], self.arr[i] = self.arr[i], self.arr[parentpos] self.indexs[parentpos], self.indexs[i] = self.indexs[i], self.indexs[parentpos] i = parentpos parentpos = (i - 1) // 2 def heappop(self): if len(self.arr) == 0: return None elif len(self.arr) == 1: item = self.arr.pop() index = self.indexs.pop() else: item = self.arr[0] self.arr[0] = self.arr.pop() index = self.indexs[0] self.indexs[0] = self.indexs.pop() self.heapify(0) return item, index Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>最小生成树</tag>
        <tag>Kruskal算法</tag>
        <tag>Prim算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图基本算法]]></title>
    <url>%2F2018%2F08%2F19%2Fbasic-graph-algorithms%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍图的基本算法，包括图的表示、深度优先搜索和广度优先搜索、拓扑排序以及强连通分量等。本文的完整代码可以在我的github找到。 图的表示图为定点和边的结合，常用图G=(V, E)来标记。图有两种常见的表示方法：邻接链表法和邻接矩阵法。邻接链表法记录一个数组Adj，数组的大小为|V|，数组的每个元素Adj[u]为一个链表，链表指向以u为起点的边。邻接矩阵法使用一个二维数组Adj来表示图，数组的维度为$|V|^2$，数组中的元素Adj[u][v]表示是否存在从u到v的边。图可以分为有向图和无向图。当一条边存在于无向图中，则其反向边也存在于无向图中，有向图则不一定。 对于权重图，邻接链接法只需在链表的结点出增加一个权重属性，邻接矩阵法则在相应的边上存放相应的权重，如果该边不存在，用0或者$\infty$表示。邻接链表法表示的图其python代码示例如下：1234567891011121314INF = float('inf')class Node: def __init__(self, v, w): self.v = v self.w = w self.pre = None self.next = Noneclass LinkGraph: def __init__(self, n): self.n = n self.adj = n * [None] 广度优先搜索对于图G=(V, E)，广度优先搜索从一个源结点s出发，系统性地遍历所有s能到达的结点。记$d[v]$为从源结点s到v所经历的最短边。在广度优先搜索总是先遍历$d[v]=k$的边，再遍历$d[v]=k+1$的边。广度优先搜索的python代码示例如下：123456789101112131415161718192021222324def bfs(self, s): d = self.n * [INF] color = self.n * ['white'] parent = self.n * [None] d[s] = 0 color[s] = 'gray' queue = [s] while queue: u = queue.pop(0) node = self.adj[u] while node: v = node.v if color[v] == 'white': color[v] = 'gray' d[v] = d[u] + 1 parent[v] = u queue.append(v) node = node.next color[u] = 'black' return parent 广度优先搜索使用一个先进先出的队列，此处使用python的列表作为队列使用。此处的广度优先搜索记录了从源结点s到v所经历的边数d[v]、v的前驱结点parent[v]。color[v]是为了避免重复访问某结点而是设置的状态数组。如果color[v]为白色，则结点v尚未被访问；如果color[v]为灰色，则结点v已经被访问，而结点v的邻接结点尚未访问完；如果color[v]为黑色，则结点v的邻接结点已经被访问完。实际上，如果没有特殊的要求，我们只需要两个状态就可以进行广度优先搜索，即黑和白。此是我们可以用一个bool数组flar[v]保存状态，初始时均为False，当访问到结点v，即将其设为True。 对于图G=(V, E)和源结点s，我们定义图G的前驱子图为$G\pi=(V\pi, E\pi), V\pi={v\in V:v.\pi \neq NIL } \bigcup{s}, E_\pi={(v.\pi, v): v\in V -{s}}$ 如果$V\pi$由从源结点s可以到达的结点组成，且对于所有$v\in V\pi$，子图$G\pi$包含一条从s到v的简单路径（不成环），且该路径为图G从s到v的最短路径（边最少），则前驱子图$G\pi$为一棵广度优先树。 广度优先搜索的parent数组所得到前驱子图即为一棵广度优先树。下图为一个无环图。 从源结点2出发进行广度优先搜索，即可得到广度优先树中的所有边，也称为树边。我们将s=2到所有结点的路径打印如下：12345672 to 0: 2-&gt;1-&gt;02 to 1: 2-&gt;12 to 3: 2-&gt;32 to 4: 2-&gt;3-&gt;42 to 5: 2-&gt;3-&gt;52 to 6: 2-&gt;3-&gt;5-&gt;62 to 7: 2-&gt;3-&gt;4-&gt;7# 深度优先搜索深度优先搜索，故名思义，总是尽可能往图的深处进行搜索。深度优先搜索的前驱子图形成一个由多棵深度优先树构成的深度优先森林。如果广度优先树，每一棵深度优先树的边也称为树边。其python代码实已如下：12345678910111213141516171819202122232425def dfs_visit(self, u, color, parent, d, f, time): time[0] += 1 d[u] = time[0] color[u] = 'gray' cur = self.adj[u] while cur: v = cur.v if color[v] == 'white': parent[v] = u self.dfs_visit(v, color, parent, d, f, time) cur = cur.next time[0] += 1 f[u] = time[0] color[u] = 'black'def dfs(self): color = self.n * ['white'] parent = self.n * [None] d = self.n * [-1] f = self.n * [-1] time = [0] for u in range(self.n): if color[u] == 'white': self.dfs_visit(u, color, parent, d, f, time) return d, f, parent深度优先搜索中d记录除此到达的时间实际上也为广度有效搜索的最短路径，记录完成时间，即对v完成深度优先搜索时的时间。color也可以如广度优先搜索只用两种状态来表示。为了将time作为引用传递，将time设为数组。对一个结点进行深度优先搜索，可以用dfsvisit过程。上述使用三种状态的颜色，其实并非完全没有作用。有以下引理&gt;一个有向图G是无环的当且仅当对其的深度优先搜索不产生后向边。对图G深度优先搜索得到的深度优先森林$G\pi$，有以下四种边的定义：1. 树边：深度优先森林$G_\pi$的边。如果结点v是因算法对边的探索而首先被发现，则(u, v)是一条树边2. 后向边：结点u连接在其在深度优先树中的一个祖先结点v的边，自循环也是后向边。3. 前向边：是结点u连接在其在深度优先树中一个后代结点v的边(u, v)。4. 横向边：除上述边之外的所有的边。只要一个结点不是另一个结点的祖先。在深度优先搜索中可以根据保存的数组信息，判断边的类型。当第一次访问边(u, v)时，结点v的颜色可以帮助我们判别1. 结点v的颜色为白色，则表明(u, v)时树边2. 结点v为灰色，边(u, v)为后向边3. 结点v为黑色，边(u, v)为前向边或者横向边。当结点v为黑色时，如何判断横向边或者前向边，可以借助d和f两个数组。首先引入括号化定理：&gt; 对于有向图或无向图进行深度优先搜索时，对于任意的两个结点u和v，以下三种情况只有一种成立：1. [u.d, u.f]和[v.d, v.f]完全分离，u不为v的后代， v不为u的后代2. [u.d, u.f]完全包含在[v.d, v.f]内，则u为v的后代。3. [v.d, v.f]完全包含在[u.d, u.f]内，则v为u的后代。上数三种情况有且仅有一种成立。因此，可以据此证明当结点v为黑色时，如果过u.d &lt; v.d，则(u, v)为前向边；当u.d &gt; v.d时，为横向边。当u.d &lt; v.d时，因为u.f &gt; v.f ，则根据括号化定理，则u为v的祖先。否则，v.d &lt; u.d, 然而v.f &lt; u.f, 则 [u.d, u.f]和[v.d, v.f]完全分离，u和v不是祖先和后代的关系。# 拓扑排序对于有向无环图G=(V, E)，拓扑排序是所有结点的一种线性次序，该次序满足以下条件：如果图包含边(u, v)，则结点u在拓扑排序中排在v的前面。拓扑排序基本思路很简单，就是对树进行深度优先搜索，每个结点的搜索完成时，将其插入到链表的头部。其python代码示例如下：123456789def topo_sort(self):_, f, _, = self.dfs()d = []for v, k in enumerate(f): d.append((k, v))d = sorted(d, key=lambda x: x[0], reverse=True)for item in d: print(item[1])这里直接调用dfs，得到其完成时间的数组f，对数组f进行降序排序，即可得到其拓扑排序。# 强连通分量对于有向图G=(V, E)的强连通分量是一个最大结点结合$c \subseteq V$, 对于该集合的任意一对结点u和v，u和v可以互相到达。求解强连通分量的基本依据为：图G中的强连通分量和图G的转置$G^T$完全相同。$G^T$即为将图G中的边全部反向得到的图。其python代码示例如下：1234567891011121314151617181920212223242526272829303132333435363738394041def reverse(self): g = LinkGraph(self.n) for i in range(self.n): cur = self.adj[i] while cur: v = cur.v g.insert(v, i, 1) cur = cur.next return gdef scc(self): g = self.reverse() flag = g.n * [False] q = [] for u in range(g.n): if not flag[u]: g.dfs_scc(u, flag, -1, q=q) count = 0 flag = self.n * [False] com = self.n * [-1] for u in q: if not flag[u]: self.dfs_scc(u, flag, count, com) count += 1 return comdef dfs_scc(self, u, flag, count, com=None, q=None): flag[u] = True if com: com[u] = count cur = self.adj[u] while cur: v = cur.v if not flag[v]: self.dfs_scc(v, flag, count, com, q) cur = cur.next if q is not None: q.insert(0, u)强连通分量并不复杂，上面的代码基本思路如下：1. 求解图G的转置图$G^T$。2. 对$G^T$进行深度优先搜索，根据结点完成时间降序排列，对图G进行深度优先搜索，并得到每一个结点对应的分量。可以看到dfs_scc和dfs_visit基本一致，只不过我们根据我们的目的做了微小的改动。我们并没有保存完成时间，而是当结点完成后，将其插入在列表q的前面，从而得到按照完成时间降序排列的结点集，然后对图G进行深度优先搜索，在深度优先搜索的过程中，为每个结点赋予其分量的编号。如果$com[i]=j$在第i个结点在第j个强连通分量上。从一个连通分量上的任意一个结点出发，该连通分量的所有点都将被访问，flag设置为True。因此，进入几次dfs_scc函数，有几个强连通分量。如果将强连通分量视为一个结点，则有强连通分量构成的图必定为有向无环图。在进行第二次深度优先搜索（即对图G进行深度优先搜索）时，按照$G^T$深度优先搜索的完成时间进行遍历的原因可见下面的图解。假设图G中存在两个强连通分量A, B，且这两个强连通分量有边相连，则从A到B的所有边的方向必然一致。如果不一致，则A, B为同一个强连通分量，与假设不符。不失一般性，我们只画出强连通分量的之间的一条边。如下图所示： 因为从A到B有一条有向边，则强连通分量A中的所有结点的完成时间都大于强连通分量B中的完成时间。则如果按照完成时间降序对$G^T$进行深度优先搜索，则必然先对强连通分量A中的结点进行深度优先搜索。$G^T$如下图所示： 若先对强连通分量A中的结点进行深度优先搜索，则B中的结点一定不会被所引索引到，从而将不同的强连通分量隔离开。当然，先对图G进行搜索还是先对图$G^T$进行搜索结果都是一样的。 假设原来的图G如下： 则其强连通分量的输出结果com如下：123456789# 下标 强连通分量0 31 32 33 14 25 16 27 0 Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>dfs</tag>
        <tag>bfs</tag>
        <tag>强连通分量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[散列表]]></title>
    <url>%2F2018%2F08%2F19%2Fhash-table%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍散列表的概念、hash函数以及散列表的冲突解决方法等。本文的完整代码可以在我的github找到。 概念散列表(Hash Table)是一种存储一对键值对(key, value)的数据结构，它可以直接根据键值key寻找元素的存储位置。散列表可以看作数组的一种推广，数组直接使用下标访问数组元素，散列表将key值映射到存储位置进而访问元素。我们定义装载因子$\alpha$如下： $$\alpha=n/m$$其中，m为散列表的槽位，n为要存储的元素数。 hash函数将key值映射到槽h(key)的函数称为hash函数。hash函数将关键字的全域U映射到散列表T[0..m-1]的槽位上： $$h: U \rightarrow {0, 1, 2, 3, …, m - 1}$$散列表的大小m一般远小于|U|。一个好的散列函数应近似地满足简单均匀散列假设： 每个关键字都被等可能的散列到m个槽位中的任何一个，并与其他关键字已散列到那个槽位无关。 常见的散列函数有除法散列和乘法散列等。下面我们将以key为整数为例，介绍hash函数。对于key为字符串等类型时，通常根据一定的规则计算出一个整数值。 除法散列法除法散列法使用模除将key映射到{0, 1, 2, 3, …, m - 1}，其python代码如下:12def div_hash(key, m): return key % m 散列表的大小m一般选择不太接近2的整数幂的素数，当$m=2^p$时， key对m取模时，相当于取key的二进制的后p位。当key的后p位的各种排列形式不是等可能的，将会增大冲突的概率。不过当后p位足够均匀时，m也可直接采用2的整数幂，而对hash值进行一系列的位变换，使得其足够均匀。例如python、java语言的源码对于hash表的实现都采用的是$m=2^p$。 乘法散列法乘法散列法使用下面的变换将key映射到{0, 1, 2, 3, …, m - 1} $$h(k)=\lfloor{m(kA mod 1)}\rfloor$$其python代码如下：12def mul_hash(key, m, A): return int(m * (A * k - int(A * k)) 其中m一般取2的整数幂，设计算机的字长位w位，A一般取形如$s/2^w$的分数，其值接近$(\sqrt{5}-1)/2$。其中kA mod 1为取kA的小数部分，即$kA - \lfloor{kA}\rfloor$。 冲突解决方法当不同的k值映射到相同的槽时，即$h(k_1)=h(k_2)$我们说发生了冲突。解决冲突的方法通常有两种：链接法和开放寻址法。 链接法链接法即使用链表来解决冲突，当发生冲突时，将映射到同一个槽的元素放在一个链表中。即对于散列表T[0, 1, 2, …, m- 1]，每一个槽i指向一个链表，该链表存放$h(k)=i$的元素。采用链接法解决冲突的python代码示例如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Node: def __init__(self, key, value): self.key = key self.value = value self.pre = None self.next =Noneclass HashTable: def __init__(self, m): self.m = m self.table = m * [None] def insert(self, key, value): h = self.div_hash(key) if self.table[h] is None: self.table[h] = Node(key, value) else: n1 = self.table[h] n2 = Node(key, value) self.table[h] = n2 n2.next = n1 n1.pre = n2 def delete(self, key): h = self.div_hash(key) n = self.table[h] while n and n.key != key: n = n.next if n is None: print('no such data with key: %s' % key) return -1 else: if n.pre is None: self.table[h] = n.next else: n.pre.next = n.next if n.next: n.next.pre = n.pre return 0 def search(self, key): h = self.div_hash(key) n = self.table[h] while n and n.key != key: n = n.next if n is None: print('no such data with key: %s' % key) return -1 else: return n.value def div_hash(self, key): h = 0 for c in key: h = ord(c) + h * 127 return h % self.m 对于散列表的每一个槽，其所指向的链表的期望长度为装载因子$\alpha$。对于链接法解决冲突的散列表，其装载因子一般大于1。在简单均匀散列的假设下，对于链接法解决冲突的散列表，一次不成功查找和一次不成功查找的平均时间均为O($1+\alpha$)。 开放寻址法开放寻址法将所有的元素存放在散列表中，每个表项包含一个元素或者为NIL。因此一般使用开放寻址法解决冲突的散列表，其装载因子$\alpha$小于1，且一般为0.75左右。开放寻址法有一次探查和二次探查等方式。 一次探查一次探查法采用的散列函数如下所示： $$h(k, i)=(h^{‘}(k)+i)modm, i=0,1, 2, …, m-1$$$h^{‘}(k)$即上述的散列函数，一次探查首先在$h^{‘}(k)$计算的位置进行探查，如果探查不到，则依次往后探查，直至探查到为止。 二次探查二次探查次采用的散列函数如下所示： $$h(k, i)=(h^{‘}(k)+c1i+c{2}i^2)modm, i=0, 1, 2, 3, …, m-1$$其中$c_1$和$c_2$为正的辅助常数，一个可能的取值为1/2， 1/2，即算法导论11-3给出的方法。以一次探查为例，其python代码示例如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Node: def __init__(self, key, value): self.key = key self.value = valueclass HashTable: def __init__(self, m): self.m = m self.table = m * [None] self.state = m * ['idle'] def div_hash(self, key): return hash(key) % self.m def linear_hash(self, key, i): return (hash(key) + i) % self.m def search(self, key): for i in range(self.m): h = self.linear_hash(key, i) if self.state[h] == 'idle': return None if self.state[h] == 'busy' and self.table[h].key == key: return self.table[h].value return None def insert(self, key, value): i = 0 while i &lt; self.m: h = self.linear_hash(key, i) if self.state[h] != 'busy': self.state[h] = 'busy' self.table[h] = Node(key, value) break i += 1 if i &gt;= self.m: print('hash map overflow') else: print('insert success') def delete(self, key): i = 0 while i &lt; self.m: h = self.linear_hash(key, i) if self.state[h] == 'busy' and self.table[h].key == key: self.state[h] = 'dirty' break i += 1 if i &lt; self.m: print('delete success') else: print('no such data with key %s' % key) 需要指出的是，在开放寻址法解决冲突的时，如果要删除某一个元素，不能直接将其删除，不然在进行查找的时候将查找不到与该元素冲突但在该元素后插入的元素。解决的办法为每一个元素加入一个状态变量，这样，当状态变量为’idle’时，表示这个位置为空；当状态变量为’dirty’时，表示这个位置的元素已经被删除；当状态变量为’busy’时，表示这个位置有元素。这样在查找时，遇到dirty的元素，并不退出循环；而在插入时，遇到dirty的元素，可以覆盖。 定义均匀散列如下：如果每个关键字的探查序列等可能地为(0, 1, …, m-1)的$m!$种排列中的任一种。在均匀散列的条件下，装载因子为$\alpha$的开放寻址散列表，一次不成功的查找，其期望的探查次数至多为$1/(1-\alpha)$，插入一个元素至多需要做$1/(1-\alpha)$次探查，一次成功查找中的期望探查次数至多为$\frac{1}{\alpha}ln(\frac{1}{1-\alpha})$。 Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树]]></title>
    <url>%2F2018%2F08%2F19%2Fred-black-tree%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍红黑树的概念、方法以及实现。本文的完整代码可以在我的github找到。 概念红黑树是一种自平衡的二叉搜索树，其满足以下性质： 每个结点或是红色或是黑色的 根结点是黑色的 每个叶结点(NIL)是黑色的 如果一个叶结点是红色的，则它的两个子结点都为黑色的 对每个结点，从该结点到其后代的所有结点的简单路径上，均包含相同数目的黑色结点 为方便起见，我们使用一个哨兵结点来代表NIL， 哨兵结点颜色属性为黑色，其他属性为任意，所有指向NIL的指针均指向该哨兵结点。红黑树具有以下性质： 一棵具有n个内部结点的红黑树的高度至多为$2log(n+1)$ 红黑树的方法红黑树具有旋转、插入、删除、最值，中序遍历等方法，其python代码示例如下：12345678910111213class Node: def __init__(self, key, color): self.parent = None self.key = key self.color = color self.left = None self.right = Noneclass RBTree: def __init__(self): self.nil = Node(-1, 'black') self.root = self.nil self.nil即为哨兵结点，其颜色为黑色，key值为-1。根结点也初始化为哨兵结点。当根结点不为哨兵结点，其父亲指向哨兵结点。 旋转旋转操作是一种能保持二叉搜索树性质的局部操作。注意，并不一定能保持红黑树性质。选中包括左旋和右旋，左旋操作假设当前结点的右孩子不为nil，右旋操作假设当前结点的左孩子不为nil。 假设当前结点为x，x的右孩子y不为nil。左旋操作沿着x-&gt;y的边进行旋转，使y变为x的父结点，由于二叉搜索树的性质，将y的左孩子给x，作为x的右孩子，y的左孩子变为x，x的父结点变为y的父结点。 假设当前结点为y，y的左孩子x不为nil。右旋操作沿着y-&gt;x的边进行旋转，使得x变成y的父结点，x的右孩子给y，使其成为y的左孩子。x的右孩子变为y，x的父结点变为y的父结点。1234567891011121314151617181920212223242526272829303132333435def left_rotate(self, x): # 假设x的右孩子不为nil y = x.right x.right = y.left if y.left != self.nil: y.left.parent = x y.parent = x.parent if x.parent == self.nil: self.root = y elif x == x.parent.left: x.parent.left = y else: x.parent.right = y y.left = x x.parent = ydef right_rotate(self, x): # 假设x的左孩子不为nil y = x.left x.left = y.right if y.right != self.nil: y.right.parent = x y.parent = x.parent if x.parent == self.nil: self.root = y elif x == x.parent.left: x.parent.left = y else: x.parent.right = y y.right = x x.parent = y 插入红黑树的插入操作与二叉搜索树类似，先查找到要插入的叶结点，然后将其插入，并将其颜色设置为红色。与二叉搜索树不同的是，插入操作可能会导致红黑树的性质无法保持。由于将新插入的结点设置为红色结点，将可能会破坏红黑树的性质2和性质4。即，如果原来红黑树为空，新插入的结点则为根结点，由于根结点为红色，破坏性质2。如果插入的结点的父结点的颜色为红，则违反性质4。插入的基本思路如下： 如同二叉搜索树的插入过程，我们先找到其应该插入的位置，将其颜色设为红色。然后进入插入的修复过程insert_fix。 如果z.p的颜色为黑色时，退出循环并设置树的根的颜色为黑色。 如果z.p的颜色为红色，且z.p = z.p.p.left时，有以下三种情况： y = z.p.p.right即为z的叔结点，当y为红色时，将z.p.p的颜色设置为红色，将z.p以及y的颜色设置黑色，令z=z.p.p，递归的进行这一过程。 当y为黑色时，此时，如果z=z.p.right，令z=z.p，左旋使得z=z.p.left，则z和z.p为红色，且z=z.p.left，转换为情况3。 当y为黑色时，z=z.p.left, 将z.p的颜色置为黑色，z.p.p置为红色，然后对z.p.p进行右旋。 如果z.p的颜色为红色，且z.p=z.p.right时，与情况3类似。只不过，左右的方向换了下。 insert_fix过程主要修复红黑树可能被破坏的性质2和性质4。其在运行过程中满足以下不变式： 若z.p为根结点，z.p为黑色结点。 结点z为红色结点。 每次迭代，只有一条红黑树性质被破坏，要么为性质2，要么为性质4。 如果当前结点z为红黑树的根结点，且z为红色，则必是将新增结点z插入到空的红黑树中，此时z.p的颜色为哨兵的颜色黑色，将不会进入循环，直接将树的根结点的颜色设置为黑色。这也是设置根结点父亲为哨兵的原因。如果z不为根结点，且违反性质4，insert_fix的局部操作保证各个分支上的黑色结点数目不变。当叔结点为红色时，令z沿着树上升，直至其父结点为黑色，z最多上升到根结点的孩子。当叔结点不为红时，此时经过一系列的左旋和右旋操作，修复性质4。此时z.p的颜色为黑，将在本次迭代后退出循环。因此，其时间复杂度为O(logn)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def insert(self, z): y = self.nil x = self.root while x != self.nil: y = x if z.key &lt; x.key: x = x.left else: x = x.right if y == self.nil: self.root = z elif z.key &lt; y.key: y.left = z else: y.right = z z.parent = y z.left = self.nil z.right = self.nil z.color = 'red' self.insert_fix(z)def insert_fix(self, z): while z.parent.color == 'red': if z.parent == z.parent.parent.left: y = z.parent.parent.right if y.color == 'red': y.color = 'black' z.parent.color = 'black' z.parent.parent.color = 'red' z = z.parent.parent else: if z == z.parent.right: z = z.parent self.left_rotate(z) z.parent.color = 'black' z.parent.parent.color = 'red' self.right_rotate(z.parent.parent) else: y = z.parent.parent.left if y.color == 'red': y.color = 'black' z.parent.color = 'black' z.parent.parent.color = 'red' z = z.parent.parent else: if z == z.parent.left: z = z.parent self.right_rotate(z) z.parent.color = 'black' z.parent.parent.color = 'red' self.left_rotate(z.parent.parent) self.root.color = 'black' 删除删除结点z的基本思路如下： 如果z的左孩子为哨兵，用y_origin_color记录z之前的颜色，用x记录将要替换z的结点，即z的右孩子x，然后用z的右孩子替换z。 如果z的右孩子为哨兵，用y_origin_color记录z之前的颜色，用x记录将要替换z的结点，即z的左孩子x， 然后用z的左孩子替换z。 如果z的左右孩子均存在，则找到z的后继，赋值为y，y_origin_color记录y的颜色，因为y为z的后继，则y必定没有左孩子，用y的右孩子替换y，用y替换z。 如果y_origin_color为黑色，则进入delete_fix过程。如果x为根结点，或者x的颜色为红色，则直接退出循环，将x的颜色设为黑色。当x不满足以上条件时，共有以下四种情况： x的兄弟结点w为红色，则w的孩子必然为黑色。交换w和x.p的颜色，并对x.p进行右旋，令w为x.p旋转后新的兄弟结点， 则w必为黑色，进入情况2，3或4。 x的兄弟结点w为黑色，且其左右孩子均为黑色，则减去x的一层黑色，为保持性质5，w也减去一层黑色，变为红色。设置x为x.p，如果新的x为红色，则退出循环，并设置x为黑色。如果新的x为黑色，则x为双重黑色，在此进入循环。由情况1进入情况2，新的x必为红色。 x的兄弟结点w为黑色，且其左孩子为红色，右孩子为黑色，则交换w.left和w的颜色，进行右旋，此时w.left即为x新的兄弟结点，且满足其右孩子为红色，进入情况4。 x的兄弟结点w为黑色，且w的右孩子为红色，则将w的颜色与x.p进行交换，对x.p进行左旋，此时w变成x.p的父结点，将w的右孩子设置成黑色，则平衡树平衡。将x设置为树的根，然后退出循环。 红黑树的删除过程的相关注解： delete过程中，y始终指向要删除的结点，x始终指向要替换y的结点。当替换完成后，x占据y的位置。当z只有一个孩子时，显而易见。当左右孩子都存在时，只需找到找到z的后继y，删除y，然后用y替换z。即将删除y变成了删除z。此时z只有一个孩子，即右孩子。 delete_fix过程中，循环过程中保持不变式，即x为双重黑色。所谓双重黑色，即为x的颜色为黑色，x取代的结点的颜色为黑色。将x设置成双重黑色，则能保持红黑树的性质5。 情况1、情况3和情况4只需进入一次循环，只有情况2才会多次进入循环，将x的双重黑色向上传递。情况1处理结束后，立刻进入情况2，情况2处理结束后，必定不满足循环条件而退出循环。情况3处理结束后立刻进入情况4，情况4处理结束后，即可退出循环。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293def transplant(self, u, v): if u.parent == self.nil: self.root = v elif u == u.parent.left: u.parent.left = v else: u.parent.right = v v.parent = u.parentdef minimum(self, node): while node.left != self.nil: node = node.left return nodedef delete(self, z): y = z y_origin_color = y.color if z.left == self.nil: x = z.right self.transplant(z, z.right) elif z.right == self.nil: x = z.left self.transplant(z, z.left) else: y = self.minimum(z.right) y_origin_color = y.color x = y.right if y.parent == z: x.parent = y else: self.transplant(y, y.right) y.right = z.right y.right.parent = y self.transplant(z, y) y.left = z.left y.left.parent = y y.color = z.color print(x.key, x.color, x.parent.key) if y_origin_color == 'black': self.delete_fix(x)def delete_fix(self, x): while x != self.root and x.color == 'black': if x == x.parent.left: w = x.parent.right if w.color == 'red': w.color = 'black' x.parent.color = 'red' self.left_rotate(x.parent) w = x.parent.right if w.left.color == 'black' and w.right.color == 'black': w.color = 'red' x = x.parent else: if w.right.color == 'black': w.color = 'red' w.left.color = 'black' self.right_rotate(w) w = x.parent.right w.color = x.parent.color x.parent.color = 'black' w.right.color = 'black' self.left_rotate(x.parent) x = self.root else: w = x.parent.left if w.color == 'red': w.color = 'black' x.parent.color = 'red' self.right_rotate(x.parent) w = x.parent.left if w.right.color == 'black' and w.left.color == 'black': w.color = 'red' x = x.parent else: if w.left.color == 'black': w.color = 'red' w.right.color = 'black' self.left_rotate(w) w = x.parent.left w.color = x.parent.color x.parent.color = 'black' w.right.color = 'black' self.right_rotate(x.parent) x = self.root x.color = 'black' 中序遍历中序遍历类似二叉搜索树，时间复杂度O(logn)。12345def midsort(self, x): if x != self.nil: self.midsort(x.left) print(x.key, x.color, x.parent.key) self.midsort(x.right) 查找查找类似二叉搜索树，时间复杂度O(logn)。1234567891011def search(self, k): cur = self.root while cur != self.nil: if k &lt; cur.key: cur = cur.left elif k &gt; cur.key: cur = cur.right else: return cur return None Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树]]></title>
    <url>%2F2018%2F08%2F19%2Fb-tree%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍B树的概念、方法以及实现。本文的完整代码可以在我的github找到。 概念m阶B树的性质为： 树中每个结点最多含有m个孩子 除根结点外，每个分支结点至少有$\lfloor{m/2}\rfloor$根子树 若根结点不是叶子结点，则其至少有两个孩子 所有的叶结点位于同一层 在算法导论中，B树的定义略有不同，一棵B树T是具有以下性质的有根树： 每个结点x具有以下属性： x.n，当前存储在结点x中的关键字 x.n个关键字本身$x.key_0, x.key_1, x.key2, …, x.key{n-1}$，以非降序存放 x.leaf，bool值。如果x为叶结点，则为TRUE; 否则为FALSE 每个结点x还包含x.n+1个指向其孩子的指针$x.c_0, x.c_1, x.c2, …, x.c{n-1}$, 叶结点没有孩子，所以它们的$c_i$属性没有定义。 每个结点x以其关键字进行分割，即$x.key_{i-1}&lt;= k &lt;= x.key_i$，则关键字k所在的结点在以$x.c_i$为根的子树上。 每个叶结点具有相同的深度，即树的高度h。 每个结点所包含的关键字个数有上界和下界。以最小度数$t&gt;=2$来表示这些界。除了根结点以外的每个结点包含的结点个数满足$t-1 &lt;= x.n &lt;= 2t-1$。 这两种定义基本是一致的，接下来，我们将主要按照算法导论上的定义来实现。B树的树高$h&lt;=\log_t\frac{n+1}{2}$。 B树的方法与二叉搜索树类似，B树主要有查找、插入与删除等算法。其python代码示例如下：1234567891011class Node: def __init__(self): self.parent = None self.keys = [] self.children = []class BTree: def __init__(self, t): self.t = t self.root = None 这里定义了如果结点为叶结点，则其children为[]。因此可以用 if node.children判断其是否为叶结点。 查找与二叉搜索树类似，B树的查找也采用递推的方式，根据B树的性质，第一个不小于k值的$x.key_i$，即为要查找的结点，或者存在于以x.children[i]为根的子树中。易知， 其时间复杂度为O(tlogn)。1234567891011121314def search(self, k): current = self.root while current: i, key = 0, current.keys[0] for i, key in enumerate(current.keys): if k &lt;= key: break if k == key: return current, i elif current.children: current = current.children[i] else: return None 插入新插入的结点总是插入在叶结点中，而插入操作可能会导致插入的叶结点其关键字超过$2t-1$，为了解决这一问题，假设此叶结点为x，我们需要将结点x分裂成两个结点，从而导致x的父节点的关键字超出其最大值$2t-1$。为了避免这种情况的发生，我们需要保证，从根结点到要插入的叶结点的路径上的结点关键字小于最大值$2t-1$。算法的借本思路如下： 如果根结点为空，则将新建结点使其成为根结点，并将关键字加入其中。否则执行步骤2。 如果根结点$len(root.keys) == 2t-1$，新增一个空结点，使其成为新的根结点，原来的根结点成为其孩子。然后分裂此结点，使其非满，并执行步骤3，否则执行步骤3。 此步骤为insert_not_full()子过程。如果当前结点为叶结点，则直接将关键字插入，否则，找到该结点对应的分支i，如果self.children[i]的关键字为2t-1，则将x的第i个结点进行分裂，并递归执行步骤3。 insert_not_full()过程始终保持着一个循环不变式：当前结点关键字为非满。因为我们每次在进入insert_not_full()过程前，都先执行其分裂过程。因此我们将分裂过程定义为对x的第i个孩子进行分裂。这也就使得当根结点关键字满时，需要新增空结点为根结点，再执行分裂过程。对x的第i个孩子进行分裂，需要将x.children[i]的最后一个关键字提至x中，这也是为什么要对路径上的每个结点进行分裂的原因。插入操作的时间复杂度也为O(tlogn)。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def insert(self, k): if self.root: if len(self.root.keys) == 2 * self.t - 1: r = self.root self.root = Node() self.root.children.append(r) r.parent = self.root self.split_node(self.root, 0) self.insert_not_full(self.root, k) else: self.insert_not_full(self.root, k) else: self.root = Node() self.root.keys.append(k)def split_node(self, x, i): z = Node() y = x.children[i] for _ in range(0, self.t - 1): z.keys.append(y.keys[self.t]) y.keys.pop(self.t) if y.children: for _ in range(0, self.t): z.children.append(y.children[self.t]) y.children.pop(self.t) x.keys.insert(i, y.keys[self.t - 1]) y.keys.pop(self.t - 1) x.children.insert(i + 1, z) z.parent = xdef insert_not_full(self, x, k): if not x.children: # 为叶结点 i = 0 while i &lt; len(x.keys) and k &gt; x.keys[i]: i += 1 x.keys.insert(i, k) else: i = 0 while i &lt; len(x.keys) and k &gt; x.keys[i]: i += 1 if len(x.children[i].keys) == 2 * self.t - 1: self.split_node(x, i) if k &gt; x.keys[i]: i = i + 1 self.insert_not_full(x.children[i], k) 删除删除操作可能会导致B树某一结点的关键字小于t-1，与删除操作类似，我们要保持从根结点到要删除的关键字的结点的路径上的结点关键字至少大于t-1。其基本思路如下： 情况1：如果此结点为叶结点，则删除关键字k 情况2：如果此结点为内部结点x，若k在x的关键字中，i为其下标，即$x.keys[i] == k$，则有以下情况： 2a) 设y为x.children[i]，即前于k的子结点。则在以y为根的子树中找到k的前驱$k^{‘}$，即最大值，然后用$k^{‘}$替换k，然后在x.children[i]中递归地删除$k^{‘}$。 2b) 设z为x.children[i + 1], (k存在，则i+1必存在)，即后于k的子结点。则在以z为根的子树中找到k的后继$k^{‘}$，即最小值。然后用$k^{‘}$替换k，然后递归地删除$k^{‘}$。 2c) 如果y和z均只有一个关键字，则将y和z合并，并将k值下移到合并的结点的关键字中。这样，新合并的结点就有2t-1个结点，从次结点中递归的删除k。 情况3：如果此结点不在当前内部结点x中，则保证包含k值的子树的根y=x.children[i]至少包含t个关键字，如果不满足，则有以下情况： 3a) 如果其相邻的兄弟都至少包含t个关键字，我们记 z为y左右兄弟中关键字最多的结点， index为其下标。将z中的一个关键字升至x中，将x.key[i]降至y中，使得y包含至少包含t个关键字。 3b) 如果相邻的兄弟关键字均少于t，即均为t-1。则将y与其相邻的一个兄弟结点合并，x.keys[i]降至合并的结点，从此结点递归的删除k。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293def delete(self, k): self.__delete(self.root, k) if len(self.root.keys) == 0: self.root = self.root.children[0] self.root.parent = Nonedef __delete(self, node, k): if node.children: i = 0 while i &lt; len(node.keys) and k &gt; node.keys[i]: i += 1 # 算法导论情况2 if k == node.keys[i]: y = node.children[i] z = node.children[i + 1] # 2a if len(y.keys) &gt;= self.t: tmp = y while tmp.children: tmp = tmp.children[-1] node.keys[i] = tmp.keys[-1] self.__delete(y, tmp.keys[-1]) # 2b elif len(z.keys) &gt;= self.t: tmp = z while tmp.children: tmp = tmp.children[0] node.keys[i] = tmp.keys[0] self.__delete(z, tmp.keys[0]) # 2c else: node.keys.pop(i) node.children.pop(i + 1) y.keys.append(k) for key in z.keys: y.keys.append(key) for child in z.children: y.children.append(child) self.__delete(y, k) # 算法导论情况3 else: y = node.children[i] if len(y.keys) == self.t - 1: if i - 1 &gt;= 0 and len(node.children[i - 1].keys)&gt;= self.t: y.keys.insert(0, node.keys[i - 1]) z = node.children[i - 1] if z.children: y.children.insert(0, z.children.pop(-1)) node.keys[i - 1] = z.keys.pop(-1) elif i + 1 &lt; len(node.children) and len(node.children[i + 1].keys) &gt;= self.t: y.keys.append(node.keys[i]) z = node.children[i + 1] if z.children: y.children.append(z.children.pop(0)) node.keys[i] = z.keys.pop(0) else: if i - 1 &gt;= 0: z = node.children[i - 1] z.keys.append(node.keys[i - 1]) for key in y.keys: z.keys.append(key) for child in y.children: z.children.append(child) node.keys.pop(i - 1) node.children.pop(i) self.__delete(z, k) if i + 1 &lt; len(node.children): z = node.children[i + 1] y.keys.append(node.keys[i]) for key in z.keys: y.keys.append(key) for child in z.children: y.children.append(child) node.keys.pop(i) node.children.pop(i + 1) self.__delete(y, k) else: self.__delete(y, k) # 算法导论情况1 else: i = 0 while i &lt; len(node.keys) and k &gt; node.keys[i]: i += 1 if k == node.keys[i]: node.keys.pop(i) Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>B树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉搜索树]]></title>
    <url>%2F2018%2F08%2F19%2Fbinary-search-tree%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍二叉搜索树的概念、方法以及实现。本文的完整代码可以在我的github找到。 概念二叉搜索树是一棵满足以下性质的树： 若x为二叉搜索树中的结点，如果y为x左子树的中的一个结点，那么y.key&lt;=x.key。如果y是x右子树的一个结点，则y.key &gt;= x.key。 对于二叉搜索树，有三种遍历方式： 前序遍历法：先访问根结点，再访问左结点（左子树，下同），最后访问右结点（右子树， 下同）。 中序遍历法：先访问左结点，再访问根结点，最后访问右结点。 后序遍历法：先访问左结点，在访问右结点，最后访问根结点。 对于二叉搜索树来说，中序遍历可以得到对树的key值进行排序。 二叉搜索树的方法二叉搜索树主要有查找、插入、删除、中序遍历、最值等方法。其python代码的实现如下：1234567891011class Node: def __init__(self, k): self.key = k self.parent = None self.left = None self.right = Noneclass Tree: def __init__(self): self.root = None 查找根据二叉搜索树的性质：设当前结点为cur，若当前结点的key值小于待查找的k，则带查找的元素在其左子树，若相等，则找到，若大于，则在其右子树。搜索时间直接取决于树的高度logn，即其时间复杂度是O(logn)。1234567891011def search(self, k): cur = self.root while cur: if k &lt; cur.key: cur = cur.left elif k &gt; cur.key: cur = cur.right else: break return cur 插入向一棵二叉搜索树中插入一个元素k，首先要查找到其要搜索的位置，类似于上述的查找过程。因此其时间复杂度也为O(logn)。需要指出的是，树的插入是插入在其叶子节点上。1234567891011121314151617181920def insert(self, k): p = None cur = self.root while cur: p = cur if k &lt; cur.key: cur = cur.left elif k &gt; cur.key: cur = cur.right else: break if p is None: self.root = Node(k) elif k &lt; p.key: p.left = Node(k) p.left.parent = p else: p.right = Node(k) p.right.parent = p 最值根据二叉查找树的性质，以node为根结点的树其最小值必然在其左子树上，最大值必然在其右子树上。其时间复杂度均为O(logn)。最大值和最小值的函数并没有直接返回其最值，而是返回其结点。因为对于以node为根结点的树，node的前驱即为其左子树的最大值结点，node的后继即为其右子树的最小值结点。1234567891011def minimum(self, node): while node.left: node = node.left return nodedef maximum(self, node): while node.right: node = node.right return node 删除二叉搜索树删除关键字的基本思路如下： 查找要删除的结点cur 如果要删除的结点没有左结点，则使用其右结点替换此结点；如果要删除的结点没有右结点，则使用其左结点替换此结点；如果此结点左右结点都存在，则用其前驱或者后继来替代。在这里，使用其后继结点来替代。transplant函数用结点v去替换结点u。因此其时间复杂度为O(logn)。12345678910111213141516171819202122232425262728293031def transplant(self, u, v): if u.parent is None: self.root = v elif u == u.parent.left: u.parent.left = v else: u.parent.right = v if v: v.parent = u.parentdef delete(self, k): cur = self.search(k) if cur is None: print('there is no value k') return -1 if cur.left is None: self.transplant(cur, cur.right) elif cur.right is None: self.transplant(cur, cur.left) else: tmp = self.minimum(cur.right) if tmp.parent != cur: self.transplant(tmp, tmp.right) tmp.right = cur.right tmp.right.parent = tmp self.transplant(cur, tmp) tmp.left = cur.left tmp.left.parent = tmp 中序遍历中序遍历的时间复杂度为O(n)，其输出序列为key值按照升序排列的数组。12345def midsort(self, t): if t: self.midsort(t.left) print(t.key, t.parent.key if t.parent else -1) self.midsort(t.right) Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>二叉搜索树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉堆]]></title>
    <url>%2F2018%2F08%2F19%2Fheap%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍二叉堆，也简称堆，为一种常见的数据结构。本文将从二叉堆的概念、二叉堆的实现、堆排序以及优先队列等进行阐述。本文的完整代码可以在我的github找到。 堆的概念堆是一个数组，它可以被看成一个近似的完全二叉树。其概念定义如下： 对于数组X = {$x_i$, i=0, 1, 2, 3, …, n}，当且仅当$xi&lt;=x{2i+1}, xi&lt;=x{2i+2}, i=0, 1, 2, 3, .., \lfloor{n}\rfloor$，数组X为小根堆，当且仅当$xi&gt;=x{2i+1}, xi&gt;=x{2i+2}, i=1, 2, 3, .., \lfloor{n}\rfloor$，数组X为大根堆。 因为堆可以看作近似的完全二叉树，因此可以证明其根结点的下标为$\lfloor{n/2}\rfloor+1, \lfloor{n/2}\rfloor+2, …, n$。注意下标为从0开始，因此可能与他处的定义略有不同。 堆的实现下面以大根堆为例，介绍堆的操作。首先是堆的调整，其python代码示例如下：12345678910111213141516def max_heapify(heap, i, n=None): if n is None: n = len(heap) left = i * 2 + 1 right = left + 1 largest = i if left &lt; n and heap[left] &gt; heap[largest]: largest = left if right &lt; n and heap[right] &gt; heap[largest]: largest = right if largest != i: heap[largest], heap[i] = heap[i], heap[largest] max_heapify(heap, largest, n) 最大堆的调整采用的是自顶向下的方法，每次调整以i为根的树，并满足以i的左右子结点为根的树已经满足大根堆的性质，交换i和其左右子节点的最大值，并对交换后的子结点进行递归处理。因为二叉堆的高为logn, 因此其时间复杂度为O(logn)。 从数组中创建大根堆即采用上述子过程，将其叶节点视为大根堆，非叶结点从后往前依次调整，使得每次调整前都满足左右结点已经为大根堆的条件。其python代码示例如下：1234def build_max_heap(heap): n = len(heap) for i in reversed(range(n // 2)): max_heapify(heap, i, n) 因为非叶结点的最大下标为$\lfloor{n/2}\rfloor$，因此从$\lfloor{n/2}\rfloor$到0依次调整数组，使其成为大根堆。此大根堆的时间复杂度为O(nlogn) 对于大根堆的push和pop操作，其python代码如下：123456789101112131415161718192021def max_heappop(heap): n = len(heap) if n == 0: return None elif n == 1: item = heap.pop() else: item = heap[0] heap[0] = heap.pop() max_heapify(heap, 0) return itemdef max_heappush(heap, item): heap.append(item) i = len(heap) - 1 parentpos = (i - 1) // 2 while i &gt; 0 and heap[parentpos] &lt; heap[i]: heap[parentpos], heap[i] = heap[i], heap[parentpos] i = parentpos parentpos = (i - 1) // 2 对于max_heappop操作，将大根堆的最大值弹出堆，并用数组最后的元素替换第0个元素，此时依然满足max_heapify的条件，直接调用max_heapify即可调整堆。对于max_heappush操作，我们将新加入的元素放至数组的最后，然后自底向上调整依次进行调整。因此这两种操作的时间复杂度均为O(logn)。 堆可用于排序，堆排序首先是原址排序，即其空间复杂度为O(1)，且其时间复杂度为O(nlogn)。其python代码示例如下：12345def max_heap_sort(heap, size): build_max_heap(heap) for i in reversed(range(1, size)): heap[0], heap[i] = heap[i], heap[0] max_heapify(heap, 0, i) 优先队列堆可以用来实现优先队列，与大根堆与小根堆相对应的是最大优先队列和最小优先队列。与上述大根堆的实现类似，我们以类的形式封装小根堆，其python 代码示例如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Heap: def __init__(self, arr=None): self.arr = arr if arr else [] if self.arr: self.buildheap() def heappush(self, item): self.arr.append(item) i = len(self.arr) - 1 parentpos = (i - 1) // 2 while i &gt; 0 and self.arr[parentpos] &gt; self.arr[i]: self.arr[parentpos], self.arr[i] = self.arr[i], self.arr[parentpos] i = parentpos parentpos = (i - 1) // 2 def buildheap(self): n = len(self.arr) for i in reversed(range(n // 2)): self.heapify(i) def heapify(self, i): n = len(self.arr) left = 2 * i + 1 right = left + 1 smallest = i if left &lt; n and self.arr[left] &lt; self.arr[smallest]: smallest = left if right &lt; n and self.arr[right] &lt; self.arr[smallest]: smallest = right if smallest != i: self.arr[smallest], self.arr[i] = self.arr[i], self.arr[smallest] self.heapify(smallest) def heappop(self): n = len(self.arr) if n == 0: return None elif n == 1: item = self.arr.pop() else: item = self.arr[0] self.arr[0] = self.arr.pop() self.heapify(0) return item Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>二叉堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法总结]]></title>
    <url>%2F2018%2F08%2F19%2Fsort%2F</url>
    <content type="text"><![CDATA[综述本文主要讲解三种经典的排序算法：插入排序，快速排序以及归并排序。本文的完整代码可以在我的github找到。 插入排序插入排序的基本思想是：假设数组arr在下标j之前的数组已经是有序的，将下标的为j的元素插入到前面的有序数组中。其python代码示例如下：12345678def insert_sort(arr): for i in range(1, len(arr)): t, j = arr[i], i - 1 while j &gt;= 0 and arr[j] &gt; arr[i]: arr[j + 1] = arr[j] j -= 1 arr[j + 1] = t 插入排序的时间复杂度可以很清晰的看出为O($n^2$)，其空间复杂度为O(1)。 快速排序快速排序的基本思想是：找到数组arr一个元素在排序之后的下标j，即当i &lt; j 时，arr[i] &lt; arr[j]；当i &gt; j 时，arr[i] &gt; arr[j]。其python代码示例如下：1234567891011121314151617def quick_sort(arr, l=None, r=None): if l is None: l = 0 if r is None: r = len(arr) - 1 if l &lt; r: i, j = l, l while i &lt; r: if arr[i] &lt; arr[r]: arr[j], arr[i] = arr[i], arr[j] j += 1 i += 1 arr[j], arr[r] = arr[r], arr[j] quick_sort(arr, l, j - 1) quick_sort(arr, j + 1, r) 从代码中可以看出其数组arr的原址上进行排序，因此也被称为原址排序法。从代码中可以看出在数组arr外只使用了常数项的额外空间，因此其空间复杂度为O(1)。时间复杂度最坏情况下，即每次原则用于划分的元素（称为主元）恰好为当前要排序元素的最值，此时每次划分得到的结果都是主元本身和其它元素。因此，最坏情况下其时间复杂度为O($n^2$)。当然如果每次的划分为相对平衡的划分，即最好情况下，其时间复杂度为O(nlogn)。代码中每次选择的主元为数组的最后一位，当然也可随机在[l, r]内选择一个元素作为主元，并将其与最后移位元素交换。 归并排序归并排序是利用分治策略的排序方法，其基本思想如下：将数组大致分为两半进行归并排序，然后再将两个已经排完序的数组进行归并。其python代码如下：123456789101112131415161718192021222324def merge_sort(arr, l=None, r=None): if l is None: l = 0 if r is None: r = len(arr) if r &gt; l + 1: m = (l + r) // 2 merge_sort(arr, l, m) merge_sort(arr, m, r) j, k = l, m t = [] for i in range(l, r): t1 = arr[j] if j &lt; m else arr[k] + 1 t2 = arr[k] if k &lt; r else arr[j] + 1 if t1 &lt; t2: t.append(t1) j += 1 else: t.append(t2) k += 1 for i in range(l, r): arr[i] = t[i - l] 从代码中可以看出，归并排序采用递归的方法对等分的子数组进行归并排序，因此其时间复杂度为O(nlogn)，空间复杂度为O(n)。 Reference本文主要参考《算法导论》。]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 CUDA9.1+cuDNN7.1安装]]></title>
    <url>%2F2018%2F04%2F16%2Fubuntu-cuda-cudnn%2F</url>
    <content type="text"><![CDATA[综述说起深度学习平台的搭建，很多人可能会想起被cuda支配的恐惧。但其实到了现在，随着Nvidia的重视，情况确实有所改善，但cuda的安装对于一个新手来说，依旧不是那么友好。本文在参考了网上的教程和英伟达的官方文档之后，结合自己的思考写下此教程。另外，本次平台搭建的环境如下： 系统 Ubuntu16.04 N卡 Nvidia GeForce GT 740M禁用Nouveau想要安装英伟达驱动，首先你需要将Nouveau驱动禁用。Linux不同发行版的禁用方式不同，根据Cuda安装文档#禁用nouveau可知，Ubuntu 版本的禁用方式如下：1sudo vi /etc/modprobe.d/blacklist-nouveau.conf 在其中添加如下两行：12blacklist nouveauoptions nouveau modeset=0 然后执行以下命令并重启:12sudo update-initramfs -usudo reboot 重新启动后可使用以下命令查看禁用是否成功1lspci | grep nouveau 如果什么都不输出，则禁用成功，反之不成功。 安装Nvidia驱动（可选）Nvidia驱动是一定要安装的，至于此处为什么要注明可选呢，其实是由于在安装Cuda时再安装Nvidia驱动也可。此处讲如何手动安装Nvidia驱动。 首先保证禁用nouveau成功，然后从英伟达驱动下载选择合适的驱动进行下载，例如，我的选择如下图所示点击搜索并下载，将其放在$home目录下，即/home/xxx/下，同时按下CTRL+ALT+F1进入tty1，输入以下命令1234sudo service lightdm stop #关闭图形界面cd ~ # 进入家目录sudo chmod a+x NVIDIA-Linux-x86_64-xxx.xx.run #修改执行权限sudo ./NVIDIA-Linux-x86_64-xxx.xx.run --no-opengl-files #运行 注意再运行时最好加上–no-opengl-files选项，防止无限重启图形界面。安装完成后，输入以下命令：1sudo nvidia-smi 如果出现类似下面的图形，则证明安装驱动成功 安装Cuda如果已经安装了Nvidia驱动的话，可以直接跳到 正式安装Cuda 环节，如果尚未安装好驱动的话，请从 安装前准备 开始 安装前准备 禁用nouveau 重启进入text mode， 参考Ubuntu16.04 进入text mode此链接，可与上一步的重启合并。 正式安装Cuda针对Ubuntu 16.04，Cuda的安装有两种形式，一种是Run installer，另一种是Deb installer，本文选用Run installer. 在Cuda下载页面选择并下载，链接包含了我的选择，为方便起见，将下载后的文件放在家目录下，注意下载补丁。例如我下载的有以下文件： cuda_9.1.85_387.26_linux.run cuda_9.1.85.1_linux.run cuda_9.1.85.2_linux.run cuda_9.1.85.3_linux.run 首先确保，这些文件都具有运行权限，如果没有，可以使用以下命令：sudo chmod a+x *.run然后运行以下命令1sudo ./cuda_9.1.85_387.26_linux.run 此时会弹出四个选项，依次为 EULA Acceptance。此为用户协议，选择accept nvidia 驱动安装。 此为安装英伟达驱动，如果已经安装过，则选择n, 未安装过则选择y Cuda 安装位置。回车，选y Samples安装位置。回车，选y 至此，安装Cuda完毕 配置Cuda路径输入以下命令，进行配置环境1234sudo vi ~/.bashrcexport PATH=/usr/local/cuda-9.1/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-9.1/lib64:$LD_LIBRARY_PATH source ~/.bashrc 安装CudnnCudnn的安装相对简单，不过对于Ubuntu16.04来说，也有两种模式，一种是deb模式，一种是手动copy模式，为保持一致，我们不选择deb模式。 在cuDNN官网下载选择与Cuda的版本相匹配的cuDNN。如我的Cuda版本是9.1, cuDNN版本是7.1，下载的文件为cudnn-9.1-linux-x64-v7.1.tgz并将其放入家目录下，输入以下命令：12345tar -xzvf cudnn-9.0-linux-x64-v7.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/includesudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64sudo chmod a+r /usr/local/cuda/include/cudnn.h \/usr/local/cuda/lib64/libcudnn* 至此cuDNN安装完毕 后记在安装完Cuda后，可以编译存储在家目录下的Nvidia-Sample目录下的示例，进行检验，具体也可参考运行Sample Reference参考的链接有： Cuda安装文档 cuDNN安装文档 知乎专栏–深度学习配置]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>cuda9.1</tag>
        <tag>cudnn7.1</tag>
        <tag>ubuntu16.04</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA算法]]></title>
    <url>%2F2018%2F03%2F30%2Fpca%2F</url>
    <content type="text"><![CDATA[综述PCA算法，即主成分分析法（Principal Components Analysis, PCA），是常用的降维方法。总的来说，降维有以下优点，这些优点通常也是我们选择降维的原因。降维的优点如下： 避免维数灾难。当数据维度增加时，要维持相同程度的密采样所需要的样本数量会成指数级增长。比如，在[0, 1]区间内每隔0.01单位采样，仅需要100个采样点。而在三维空间中，如果将0.01小方格为采样间隔，则需要$10^6$个采样点。当维度更高时，采样点数量将极其庞大。 去除噪声。采样数据往往会带有噪声。以x、s、n分别表示采样数据、原始信号、噪声信号。通常可记其关系如下：$x = s + n$。由于PCA降维是保留方差最大的方向，而相对而言噪声信号n的方差要小于真实信号s的方差，因此关于噪声信号的相关信息会被滤除。 去除冗余。数据的各个维度可能并不完全独立，通过降维通过保留主要的维度从而去除冗余。 PCA算法原理及推导从数据压缩的角度来看，我们希望压缩后的数据能够尽可能的保留更多的信息。以下为推导过程 样本数据集{$x_i$, i=1, 2, 3, …, m}，其中$x_i$为n维的列向量, 且已经去均均值化。${w_i, i=1, 2, 3, …, n}$表示特征空间的一组正交基, 其中$w_i$为n维列向量。令$W=[w_1, w_2, …, W_d]$表示由正交基中的d个基构成的矩阵，则W为$n*d$的矩阵。此时， $$z_i=W^Tx_i \tag {1}$$$$\hat{x_i}=Wz_i \tag {2}$$最小化信息损失，则有 $$min \sum_{i=1}^m {\left|\right|Wz_i-x_i\left|\right|}^2 \tag {3}$$将上式分解开来，则有 $$min -\sum_{i=1}^m z_i^Tz_i \tag {4}$$此处分解忽视了只有$x_i$所引入的常量。由内积与矩阵的迹的对应关系$z_i^Tz_i=tr(z_iz_i^T)$，即可得 $$min -\sum_{i=1}^m tr(z_iz_i^T) \tag {5}$$ $$min -\sum_{i=1}^m tr(W^T(x_ix_i^T)W) \tag {6}$$由于W由正交基构成，所以有 $$W^TW=I$$又$\sum_i^mx_ix_i^T=XX^T$，即有如下优化 $$min -tr(W^TXX^TW) \tag {7}$$$$W^TW=I \tag {8}$$由拉格朗日乘子法可得 $$XX^TW=\lambda W \tag {9}$$ 从方差最大化的角度，即我们希望降维后的样本尽可能的分开，可以得到同样的结果。从以上的推导过程中可以很容易的得到PCA的算法实现过程。 对数据去均值化 求协方差矩阵$A=\sum_i^mx_ix_i^T=XX^T$ 对协方差矩阵A进行奇异值分解，选取特征值较大的d个特征值所对应的特征向量，即可得到W 由W可求出降维后的数据$Z=W^TX$ 核化PCA算法以上的PCA算法为线性的，有时我们需要引入非线性关系进行降维，这时，核技巧就派上用场了。下面我们来介绍核化PCA降维。 首先引入非线性关系$\phi$，由(9)式可得 $$XX^TW=(\sum_{i=1}^{m}\phi(x_i)\phi(x_i)^T)W=\lambda W \tag {10}$$ 则有， $$W=\frac{1}{\lambda}(\sum_{i=1}^{m}\phi(x_i)\phi(x_i)^T)W=\sum_{i=1}^{m}\phi(x_i)\frac{\phi(x_i)^TW}{\lambda}\tag {11}$$ 记$\alpha_i=\frac{1}{\lambda}\phi(x_i)^TW$，则$\alpha_i$为d维行向量，则有 $$W=\sum_{i=1}^{m}\phi(x_i)\alpha_i \tag {12}$$ 核技巧往往是已知核化后的内积，而非知道核本身，即 $$\kappa(x_i, x_j)=\phi(x_i)^T\phi(x_j)=\phi(x_j)^T\phi(x_i) \tag {13}$$ 将式(12), (13)带入(10)式即可得到 $$\sum_{i=1}^m\phi(x_i)(\sum_{j=1}^m\kappa(x_i, x_j)\alpha_j-\lambda\alpha_i)=0\tag {14}$$ 令$K_{ij}=\kappa(x_i, x_j)$，$A=[\alpha_i;…;\alpha_m]$，则A为m*d的矩阵，有 $$KA=\lambda A \tag {15}$$将K进行奇异值分解即可得到${\alpha_i, i=1,2,…,m}$，下面来整理一下维度，$x_i$为n维列向量，则X为$n*m$的矩阵，$\alpha_i$为d维行向量，则A为$m*d$的矩阵，K为$m*m$的矩阵，则KA仍然为$m*d$的矩阵。令$z_i=W^T\phi(x_i)$, 则$z_i$为$d*1$的向量。核化PCA将$n*1$的向量降维为$d*1$的向量。则 $$z_j=W^T\phi(x_j)=\sum_{i=1}^{m} \alpha_i^T \phi(x_i)^T x_j=\sum_{i=1}^{m} \alpha_i^T \kappa(x_i, x_j) \tag {16}$$将其向量化即可得到 $$Z=A^TK \tag {17}$$ 以上即为KPCA算法的原理及推导，其算法流程如下： 对数据去均值化 求协方差矩阵$K$，$K_{ij}=\kappa(x_i, x_j)$ 对协方差矩阵K进行奇异值分解，选取特征值较大的d个特征值所对应的特征向量，即可得到A 由W可求出降维后的数据$Z=A^TK$ PCA及KPCA实现根据以上的算法过程，佐以numpy的函数库， 可以很轻易地实现PCA算法，其Python实现如下：12345678910111213141516171819202122232425262728293031323334def PCA(X, dim): ''' PCA算法实现 :param X:X.shape = (m, n), n表示特征维度，m表示样本数量 :param dim:表示转换后的维度 :return:W(n*d), Z(m*d) W为转换矩阵，Z为转换后的数据 ''' # 均值化 X = np.array(X) - np.mean(X, 0) A = np.matmul(X.T, X) u, s, v = np.linalg.svd(A) W = v[:, 0:dim] Z = np.matmul(X, W) return W, Zdef KPCA(X, dim, kernel, sigma): ''' KPCA算法实现 :param X: X.shape = (m, n), n表示特征维度，m表示样本数量 :param dim: 表示转换后的特征维度 :param kernel: 核方法 :param sigma: 此处使用高斯核，因此直接显式表示出来了 :return: A(m*d), Z(m*d) A为转换矩阵，Z为转换后的数据 ''' X = np.array(X) - np.mean(X, 0) K = [[kernel(xi, xj, sigma) for xi in X] for xj in X] u, s, v = np.linalg.svd(K) A = v[:, 0:dim] Z = np.matmul(K, A) return A, Zdef gaussian(xi, xj,sigma): return 1./sigma/np.sqrt(2\*np.pi)\*np.exp(-np.sum(np.square(np.subtract(xi, xj)))/sigma\*\*2) 参考文献主要参考文献为周志华老师的《机器学习》西瓜书]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>降维算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zsh、tmux和vim插件的配置和使用]]></title>
    <url>%2F2017%2F12%2F18%2Fzsh-vim-tmux-config%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍了配置zsh，安装配置vim插件，以及安装tmux等。 安装配置zsh安装zshDebian系可以直接通过以下命令安装zsh:1sudo apt-get install zsh 此时，执行以下命令123# cat /etc/shells # 查看所有shellecho $SHELL # 查看默认shell# chsh -c $(which zsh) # 修改默认shell 通过查看默认shell,发现没什么变化，别急，这才刚刚开始。网上有大神写了配置zsh的脚本文件，为大多数人所采纳，不必担心shell中注入恶意代码，安装方式如下：1sh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" 安装完之后，重启所有终端即可进入zsh。 配置zsh安装powerlevel9k在终端下输入以下命令1git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k 即安装完毕 安装awesome-terminal-fonts在终端下执行以下命令1git clone https://github.com/gabrielelana/awesome-terminal-fonts.git 将fonts clone到本地，然后执行以下命令，将awesome-terminal-fonts/build目录下的文件拷贝到~/.fonts目录下，如果没有~/.fonts目录则创建一个。命令如下：123456cd awesome-terminal-fontscp ./build/* ~/.fonts/*fc-cache -fv ~/.fontscp ./config/10-symbols.conf ~/.config/fontconfig/conf.d# 若~/.config/fontconfig/conf.d不存在，则按照如下命令创建# mkdir ~/.config/fontconfig/conf.d -p 执行完上述命令后，打开.zshrc文件，在其中加入以下内容：1source ~/.fonts/*.sh 配置zsh打开.zshrc，在其中添加或修改以下内容，注意POWERLEVEL9K_MODE=’awesome-fontconfig’需放置在ZSH_THEME=”powerlevel9k/powerlevel9k”之前。12345678910 6 export TERM=&quot;xterm-256color&quot; 7 # Set name of the theme to load. Optionally, if you set this to &quot;random&quot; 8 # it&apos;ll load a random theme each time that oh-my-zsh is loaded. 9 # See https://github.com/robbyrussell/oh-my-zsh/wiki/Themes10 # ZSH_THEME=&quot;robbyrussell&quot;11 POWERLEVEL9K_MODE=&apos;awesome-fontconfig&apos;12 ZSH_THEME=&quot;powerlevel9k/powerlevel9k&quot;13 POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(os_icon dir vcs)14 POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status custom_git_stats time)17 POWERLEVEL9K_OS_ICON_FOREGROUND=&quot;blue&quot; 13-17行定义了zsh的prompt的样式，保存并退出，并在终端下执行source ~/.zshrc即可看到酷炫的zsh样式。如果有出现乱码，可先确定awesome-terminal-fonts是否安装出错，若已确认正确安装，仍出现乱码，可按照链接4安装Powerline fonts。并将POWERLEVEL9K_MODE=’awesome-fontconfig’注释掉。 配置vim安装vundle在终端下执行以下命令：1git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 即可安装vundle. 安装插件打开.vimrc，如果没有则新建，在其中添加以下内容：12345678910111213141516171819202122232425262728293031323334353637383940414243444546 1 set nocompatible 2 set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936 3 set termencoding=utf-8 4 set encoding=utf-8 5 set nu 6 &quot; 设置默认进行大小写不敏感查找 7 set ignorecase 8 &quot; 如果有一个大写字母，则切换到大小写敏感查找 9 set smartcase10 &quot; vundle 环境设置11 filetype off12 set rtp+=~/.vim/bundle/Vundle.vim13 &quot; vundle 管理的插件列表必须位于 vundle#begin() 和 vundle#end() 之间14 call vundle#begin()15 Plugin &apos;VundleVim/Vundle.vim&apos;16 Plugin &apos;scrooloose/nerdtree&apos; &quot; file/directory treee17 Plugin &apos;scrooloose/nerdcommenter&apos; &quot; code commenter18 Plugin &apos;kien/ctrlp.vim&apos; &quot; Fuzzy file, buffer, mru, tag, etc finder19 Plugin &apos;altercation/vim-colors-solarized&apos;20 Plugin &apos;tomasr/molokai&apos;21 Plugin &apos;jacoborus/tender&apos;22 Plugin &apos;Shougo/vimproc.vim&apos;23 &quot; 插件列表结束24 call vundle#end()25 filetype plugin indent on26 &quot; 配置 nerdtree27 autocmd vimenter * NERDTree # 添加此行即可使用nerdtree28 map &lt;C-n&gt; :NERDTreeToggle&lt;CR&gt; # 将打开nerdtree的快捷键定义为ctrl+n29 &quot; 配置 ctrlp30 let g:ctrlp_map = &apos;&lt;c-p&gt;&apos; # 定义ctrlp快捷键为 ctrl + p31 let g:ctrlp_cmd = &apos;CtrlP&apos; # 定义ctrlp命令为CtrlP32 let g:ctrlp_working_path_mode = &apos;ra&apos; # 定义ctrlp搜索模式为ra33 &quot; 配置 nerdcommenter34 let g:NERDSpaceDelims = 1 # 定义nerdcommenter默认在注释符号后面加一空格35 &quot; 配置 vim-colors-solarized36 &quot; syntax enable 37 &quot; set background=dark38 &quot; colorscheme solarized39 &quot; 配置 molokai40 &quot; let g:molokai\_original = 141 &quot; let g:rehash256 = 142 &quot; 配置 tender 43 syntax enable # 打开符号高亮44 colorscheme tender # 定义配色方案45 &quot; colorscheme molokai46 &quot; colorscheme solarized 10–25行定义了vundle的设置，vim插件需置于 vundle#begin() 和 vundle#end() 之间。27–28行定义了nerdtree的配置，可参照每行后的注释符号了解其意义。30–32定义ctrlp的配置。nerdcommenter和scheme的设置也如上所示。另外，[vim插件库]https://vimawesome.com/包含了大量的插件，及其安装和配置方法。 相关插件使用方法nerdtree 插件使用方法nerdtree 快捷键如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445ctrl + w + h 光标 focus 左侧树形目录ctrl + w + l 光标 focus 右侧文件显示窗口ctrl + w + w 光标自动在左右侧窗口切换ctrl + w + r 移动当前窗口的布局位置o 展开当前所在目录O 递归的展开当前所在目录下的所有目录x 合拢选中结点的父目录X 递归 合拢选中结点下的所有目录i (水平地)split 一个新窗口打开选中文件，并跳到该窗口gi split 一个新窗口打开选中文件，但不跳到该窗口s (竖直地)vsplit 一个新窗口打开选中文件，并跳到该窗口gs vsplit 一个新 窗口打开选中文件，但不跳到该窗口 P 跳到根结点p 跳到父结点K 跳到当前目录下同级的第一个结点J 跳到当前目录下同级的最后一个结点k 跳到当前目录下同级的前一个结点j 跳到当前目录下同级的后一个结点C 将选中目录或选中文件的父目录设为根结点u 将当前根结点的父目录设为根目录，并变成合拢原根结点U 将当前根结点的父目录设为根目录，但保持展开原根结点m 显示文件系统菜单，然后根据提示进行文件的操作如新建，重命名等q 关闭 NerdTree 窗口? 切换是否显示 Quick Helpe Edit the current difI 切换是否显示隐藏文件f 切换是否使用文件过滤器F 切换是否显示文件B 切换是否显示书签:tabnew [++opt选项] ［＋cmd］ 文件 建立对指定文件新的tab:tabc 关闭当前的 tab:tabo 关闭所有其他的 tab:tabs 查看所有打开的 tab:tabp 前一个 tab:tabn 后一个 tabgT 前一个 tabgt 后一个 tab 上面指令的前后顺序大致可以反映命令的使用频率从高到低的顺序。 ctrlp 使用方法ctrl + p 快捷键，在vim命令模式下，可以进行模糊查找，直接输入关键字即可查找，在插入模式下，ctrl + p 快捷键可以自动补全。 nerdcommenter 使用方法nerdcommenter 快捷键默认前缀为\，常用快捷键为12\cc 对当前行进行注释 \cu 对当前行去注释 配合vim ctrl + v在可视模式下，光标移动选择多行，可以进行多行的注释和去注释。 tmux安装及使用安装tmuxdebian系可直接使用apt-get进行安装，由于apt-get的版本相对较老，本文此案去了手动编译的方式，由于tmux依赖于libevent 2.x和ncurses，因此需要先编译安装以上两个依赖。分别在http://libevent.org和http://libevent.org下载压缩包，加压分别命名为libevent、ncurses，进入libevent，执行以下命令：12345cd libevent./autogen.sh./configuremakesudo make install 进入ncurses执行以下命令：12345cd ncursesexport CPPFLAGS=&quot;-P&quot; # 如果不配置这个选项，会报错./configuremakesudo make install 安装完成后，执行以下命令，12345git clone https://github.com/tmux/tmux.gitcd tmuxsh autogen.sh./configure &amp;&amp; makesudo make install 此时运行tmux，可能会报/usr/local/lib/libevent-2.1.so.6 cannot open shared object file: No such file or directory的错误，这是因为libevent安装在了/usr/local/lib中，而非/usr/lib中，此时可以执行下命令1sudo ln -s /usr/local/lib/libevent-2.1.so.6 /usr/lib/libevent-2.1.so.6 或者也可以直接拷贝进去1sudo cp /usr/local/lib/libevent-2.1.so.6 /usr/lib/ 至此，tmux已经安装完毕 配置tmux在终端下打开~/.tmux.conf，如果没有则新建，在其中添加以下内容：123456789101112131415161718192021222324252627282930313233# 基本设置 1 set -g default-terminal &quot;screen-256color&quot; 3 set -g display-time 3000 4 set -g escape-time 0 5 set -g history-limit 65535 6 set -g base-index 1 7 set -g pane-base-index 1 8 # 9 # # 前缀绑定 (Ctrl+a) 10 set -g prefix ^a 11 unbind ^b 12 bind a send-prefix 13 # 14 # # 分割窗口 15 unbind &apos;&quot;&apos; 16 bind - splitw -v 17 unbind % 18 bind | splitw -h 19 # 20 # # 选中窗口 21 bind-key k select-pane -U 22 bind-key j select-pane -D 23 bind-key h select-pane -L 24 bind-key l select-pane -R 25 # 26 # # copy-mode 将快捷键设置为 vi 模式 27 setw -g mode-keys vi 28 # 29 # # 启用鼠标(Tmux v2.1) 30 set -g mouse on 31 # 32 # # 更新配置文件 33 bind r source-file ~/.tmux.conf \; display &quot;已更新&quot; 以上设置主要将tmux的前缀改为Ctrl+a，并设置-和|来进行横向和竖向分屏，比较直观 使用tmux在tmux外，或者说shell中，tmux有以下命令12345tmux [new -s 会话名 -n 窗口名] # 启动新会话 tmux at [-t 会话名] # 恢复会话tmux ls # 列出所有huihuatmux kill-session -t 会话名 # 关闭会话tmux kill-server # 关闭所有会话和窗口 在tmux内部，按下前缀ctrl + a(在conf中定义的前缀，默认为ctrl + b)，有以下快捷键和命令：1234567891011121314151617181920212223242526&quot;&quot; 会话:new&lt;回车&gt; 启动新会话s 列出所有会话$ 重命名当前会话&quot;&quot; 窗口c 创建新窗口w 列出所有窗口n 后一个窗口p 前一个窗口f 查找窗口, 重命名当前窗口&amp; 关闭当前窗口&quot;&quot; 窗格(分割窗口)| 垂直分割\- 水平分割o 交换窗格x 关闭窗格⍽ 左边这个符号代表空格键 - 切换布局q 显示每个窗格是第几个，当数字出现的时候按数字几就选中第几个窗格&#123; 与上一个窗格交换位置&#125; 与下一个窗格交换位置z 切换窗格最大化/最小化 ← 左移窗格→ 右移窗格↑ 上移窗格↓ 下移窗格 我经常使用的快捷键多为ctrl + a，然后[1 - 9] 切换窗口，| -分割窗格，↑ ↓ ← → 移动窗格，c 创建窗口，&amp; 关闭窗口等。更多命令，可以参考链接http://wdxtub.com/2016/03/30/tmux-guide/。 Reference本文主要参考了以下链接： oh-my-zsh powerlevel9k awesome-terminal-fonts Powerline Fonts tmux vundle vim插件库]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>zsh</tag>
        <tag>vim</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用hexo和github搭建自己的博客]]></title>
    <url>%2F2017%2F12%2F17%2Fhexo-github-pages%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍如何利用github以及hexo搭建自己的博客，以及一些第三方插件的使用。 安装Nodejs和hexo由于我使用的是ubuntu 16.04，debian系的系统可以采用如下方式安装：12wget -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -sudo apt-get install -y nodejs 然后，将npm的源换成淘宝的源：1npm config set registry https://registry.npm.taobao.org hexo的安装方式如下：1npm install hexo-cli -g 创建并部署到github pages首先创建一个github repository, 默认即可，名字可命名为xxx.github.io，名字可以根据自己的心意变更，不拘泥于形式。然后，在家目录(根据自己的习惯)下，执行以下命令：1234mkdir xxx.github.iocd xxx.github.iohexo initnpm install 这样就可以初始化完毕了你的博客文件夹，此时可以执行以下命令：1hexo clean &amp;&amp; hexo g &amp;&amp; hexo s 在浏览器中输入 http://localhost:4000/ 即可看到你的博客页面。那么，如何部署到github上呢？首先，需要在wxx.github.io文件夹下执行以下命令：1npm install hexo-deployer-git --save 请确保你的电脑上安装有git，如果没有，debian系下可执行以下命令：1sudo apt-get install git 之后，在xxx.github.io文件下打开_config.yml，进行如下配置:1234567891011121314 5 # site 6 title: your blog title # 你的网页title 7 subtitle: 8 description: your description # 你的描述 9 author: your name # 你的名字或者昵称 10 language: zh-Hans # 你的语言设置 11 timezone: Asia/Shanghai # 你的时区设置...... 77 # Deployment 78 ## Docs: https://hexo.io/docs/deployment.html 79 deploy: 80 type: git 81 repository: https://github.com/xxx/xxx.github.io.git 82 bracnch: master 将your xxx 设置成whatever you want， 然后deploy下面的type设置成git, 将repository设置成在刚才github上新建的repository，branch设置成master。然后在xxx.github.io目录下执行1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 在浏览器中输入 xxx.github.io，即可访问你的网站。 更换hexo主题安装next主题链接https://hexo.io/themes/列出了hexo的一些主题，你可在上面寻找自己喜欢的主题，然后更换。本文选择了应用较广的主题next，以下是其配置方法。在此之前，需要先明确两个文件，一个称之为主题配置文件，另一个称之为站点配置文件。以下xxx.github.io的文件结构：12345678910xxx.github.io├── _config.yml├── db.json├── node_modules├── package.json├── package-lock.json├── public├── scaffolds├── source└── themes 在xxx.github.io下的_config.yml称之为站点配置文件，在themes/next/文件夹下的_config.yml称之为主题配置文件夹。不用纠结自己的themes文件夹下没有next，我们现在就开始安装。打开终端，执行以下命令：12cd xxx.github.iogit clone https://github.com/iissnan/hexo-theme-next themes/next 同样也可以https://github.com/iissnan/hexo-theme-next下载zip文件，然后将其解压到themes文件夹下，并将其重命名为next。接着对站点配置文件，作如下配置：175 theme: next 此时在xxx.github.io目录下运行以下命令1hexo clean &amp;&amp; hexo g &amp;&amp; hexo s 即可在浏览器下看到next主题的页面 next主题设置选择scheme在主题配置文件，即themes/next/_config.yml中，将Scheme进行如下设置：1108 scheme: Pisces 设置菜单在主题配置文件中，将menu进行如下设置：123456menu: home: / || home # about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive 此时还需在xxx.github.io文件夹下执行以下命令：12hexo new page tagshexo new page categories 并在xxx.github.io/source/categories下的index.md中添加以下信息：12type: &quot;categoriescomments: false 同理，在xxx.github.io/source/tags下的index.md中添加以下信息：12type: &quot;tags&quot;comments: false 设置comments为false是为了让以后添加的评论功能在这两个page下不显示。 配置头像及站点标志在主题配置文件中，修改字段avatar如下所示：1avatar: /path/to/your/avatar 将/path/to/your/avatar设置成你的图片所在的路径，如我在xxx.github.io/source/下新建了images文件夹，并将我的图片放在其中，路径设置为/images/avatar.jpg，注意不要加source。同样在主题配置文件中，设置favicon字段如下所示：12345favicon: small: /images/avatar.jpg medium: /images/avatar.jpg apple_touch_icon: /images/avatar.jpg safari_pinned_tab: /images/avatar.jpg 集成第三方服务关于更多主题配置和第三方服务设置，请参考链接[next主题配置]http://theme-next.iissnan.com/theme-settings.html以及[next第三方服务集成]http://theme-next.iissnan.com/third-party-services.html Reference本文主要参考了以下链接： next 官网–主题配置 next 官网–第三方服务 hexo 官网 nodejs 官网 https://github.com/isLishude/blog/issues/62]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install caffe on ubuntu16.04]]></title>
    <url>%2F2017%2F12%2F17%2Fcaffe-cpu-only-with-ubuntu1604-md%2F</url>
    <content type="text"><![CDATA[综述本文主要介绍在ubuntu 16.04 系统上编译安装caffe，请注意这里是CPU ONLY版本。 安装依赖12345678910sudo apt-get updatesudo apt-get upgradesudo apt-get install -y build-essential cmake git pkg-configsudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev \libhdf5-serial-dev protobuf-compilersudo apt-get install -y libopenblas-dev liblapack-dev libatlas-base-devsudo apt-get install -y --no-install-recommends libboost-all-devsudo apt-get install -y libgflags-dev libgoogle-glog-dev liblmdb-devsudo apt-get install -y python3-dev #已有可不必安装sudo apt-get install -y python3-numpy python3-scipy 其中 -y 选项可以省略 安装opencv 3.2.0安装依赖123456[compiler] sudo apt-get install build-essential[required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev \libavformat-dev libswscale-devsudo apt-get install liblapacke-dev checkinstall #1[optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev \libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev 其中，compiler和required必选，optional可选，注释#1的行需要安装，不然会引发一个错误，这个错误是fatal error: LAPACKE_H_PATH-NOTFOUND/lapacke.h: No such file or directory#include “LAPACKE_H_PATH-NOTFOUND/lapacke.h”这个错误很奇葩，产生这个错误感觉是脚本的锅，如果使用了sudo apt-get install liblapacke-dev checkinstall还出现这个错误，那么就手动找到报错的文件，将#include “LAPACKE_H_PATH-NOTFOUND/lapacke.h”修改为#include “lapacke.h” 下载opencv3.2.0源码可以使用git下载123cd ~/&lt;my_working_directory&gt;git clone https://github.com/opencv/opencv.gitgit clone https://github.com/opencv/opencv_contrib.git 也可以下载zip文件解压，解压后，为方便起见，将opencv-3.2.0,重命名为opencv，将opencv_contrib重命名为contrib,并将其拷贝至opencv目录下 编译源码首先创建build目录123cd opencvmkdir buildcd build 然后编译源码12345678sudo cmake -DCMAKE_BUILD_TYPE=Release \-DCMAKE_INSTALL_PREFIX=/usr/local \-DPYTHON3_EXECUTABLE=/usr/bin/python3 \-DPYTHON_INCLUDE_DIR=/usr/include/python3.5 \-DPYTHON_INCLUDE_DIR2=/usr/include/x86_64-linux-gnu/python3.5m \-DPYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3.5m.so \-DPYTHON3_NUMPY_INCLUDE_DIRS=/usr/lib/python3/dist-packages/numpy/core/include/ \-DOPENCV_EXTRA_MODULES_PATH=/home/dsk/dsk/opencv/contrib/modules .. 注意，将python3或者3.5设置成你自己的python版本，注意，-Dkey=value形式key和value之间不要有空格，否则会提示-DOPENCV_EXTRA_MODULES_PATH解析错误，由于这个空格浪费了我不少时间如果遇到要下载文件，可能由于墙的原因，而下载失败，可以手动下载然后将文件拷贝至此文件夹下opencv/contrib/modules/dnn/.download/bd5e3eed635a8d32e2b99658633815ef/v3.1.0，可以使用命令行拷贝以上cmake设置成功之后，开始编译12sudo make -j4sudo make install 注意 make -j4是指用4个线程同时运行。可先用nproc查看可用的处理器数目 安装caffe下载源码并设置相关文件下载caffe 源码，解压，进入caffe文件夹，将Makefile.config.example拷贝为Makefile.config123cd caffe-mastercp Makefile.config.example Makefile.configsudo gedit Makefile.config 在gedit上将以下内容修改，将以下行的注释去掉USE_CUDNN := 0OPENCV_VERSION := 3由于我使用python 3，所以将python2全部注释掉，将python3的注释去掉 而且，将 # Whatever else you find you need goes here. 下面的12INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib 分别修改为：123INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu \/usr/lib/x86_64-linux-gnu/hdf5/serial 主要是因为ubuntu16.04文件位置 与以前版本相比，发生了变化，而系统需要hdf5的位置，因此需要修改路径.最后，打开makefile文件，做如下修改，将1NVCCFLAGS +=-ccbin=$(CXX) -Xcompiler-fPIC $(COMMON_FLAGS) 替换为：1NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS) 编译源码最后，使用以下命令编译源码12345make all -j4make test -j4make runtest -j4make pycaffe -j4make distribute -j4 如果以上没有报错，那么恭喜你，如果出现以下错误，可以进行如此修改： not found lboost_python3 解决方法，修改Makefile.config文件，将123PYTHON_LIBRARIES := boost_python3 python3.5mPYTHON_INCLUDE := /usr/include/python3.5m \ /usr/lib/python3.5/dist-packages/numpy/core/include 修改为123PYTHON_LIBRARIES := boost_python-py35 python3.5mPYTHON_INCLUDE := /usr/include/python3.5m \ /usr/lib/python3.5/dist-packages/numpy/core/include not found libopencv_core.so.3.2.0 这个为ld_library的问题，解决方法为：1sudo vi /etc/ld.so.conf.d/opencv.conf 添加以下内容1/usr/local/lib /usr/local/lib为我的ibopencv_core.so.3.2.0所在的文件夹，你需要修改为你的相应文件夹 Reference本文主要参考了以下链接： https://github.com/BVLC/caffe/wiki/Ubuntu-16.04-or-15.10-Installation-Guide https://docs.opencv.org/3.2.0/d7/d9f/tutorial_linux_install.html http://www.linuxidc.com/Linux/2016-12/138870.htm和http://www.linuxidc.com/Linux/2016-12/138870p2.htm]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>python3</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
</search>
